{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w2hrA8Gnpqo"
      },
      "source": [
        "# Importação de bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nXNwb6hnsuX",
        "outputId": "2d8f93e2-bb58-4555-b795-b6cf8667a4b6"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas --upgrade\n",
        "# !pip install numpy --upgrade\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gE1iFNwjg5M"
      },
      "source": [
        "## Importação do dataset (walmart-recruiting-store-sales-forecasting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-_3IZ4aPgfHk",
        "outputId": "55ba9edf-1030-401d-b28a-074b71968d7c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Definir o caminho base para os datasets raw no GitHub\n",
        "github_base_url = 'https://raw.githubusercontent.com/gustavolima007/demand-predictor-walmart-MLOps/main/data/walmart-recruiting-store-sales-forecasting/'\n",
        "\n",
        "# Lista para armazenar os dataframes\n",
        "dfs = {}\n",
        "\n",
        "# Nomes dos arquivos CSV a serem importados\n",
        "csv_files = [\n",
        "    'features.csv',\n",
        "    'sampleSubmission.csv',\n",
        "    'stores.csv',\n",
        "    'test.csv',\n",
        "    'train.csv'\n",
        "]\n",
        "\n",
        "# Importar cada arquivo CSV do GitHub\n",
        "for file_name in csv_files:\n",
        "    file_url = github_base_url + file_name\n",
        "    df_name = file_name.replace('.csv', '') # Nome do dataframe será o nome do arquivo sem a extensão\n",
        "    try:\n",
        "        dfs[df_name] = pd.read_csv(file_url)\n",
        "        print(f\"Arquivo {file_name} importado como dataframe '{df_name}' do GitHub.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao importar o arquivo {file_name} do GitHub: {e}\")\n",
        "\n",
        "# Agora, acesse e exiba as 5 primeiras linhas de cada dataframe\n",
        "print(\"\\nExibindo as 5 primeiras linhas de cada dataframe:\")\n",
        "\n",
        "# Acesse o dataframe 'stores' e exiba as 5 primeiras linhas\n",
        "df_stores = dfs.get('stores')\n",
        "if df_stores is not None:\n",
        "    print(\"\\nExibindo dataframe 'df_stores':\")\n",
        "    display(df_stores.head())\n",
        "\n",
        "# Acesse o dataframe 'train' e exiba as 5 primeiras linhas\n",
        "df_train = dfs.get('train')\n",
        "if df_train is not None:\n",
        "    print(\"\\nExibindo dataframe 'df_train':\")\n",
        "    display(df_train.head())\n",
        "\n",
        "# Acesse o dataframe 'test' e exiba as 5 primeiras linhas\n",
        "df_test = dfs.get('test')\n",
        "if df_test is not None:\n",
        "    print(\"\\nExibindo dataframe 'df_test':\")\n",
        "    display(df_test.head())\n",
        "\n",
        "# Acesse o dataframe 'sampleSubmission' e exiba as 5 primeiras linhas\n",
        "df_sampleSubmission = dfs.get('sampleSubmission')\n",
        "if df_sampleSubmission is not None:\n",
        "    print(\"\\nExibindo dataframe 'df_sampleSubmission':\")\n",
        "    display(df_sampleSubmission.head())\n",
        "\n",
        "# Acesse o dataframe 'features' e exiba as 5 primeiras linhas\n",
        "df_features = dfs.get('features')\n",
        "if df_features is not None:\n",
        "    print(\"\\nExibindo dataframe 'df_features':\")\n",
        "    display(df_features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwCJb6i0jpZL"
      },
      "source": [
        "## Unificação dos Dados (Merge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "Gy1DS-_Zj6Lo",
        "outputId": "83066427-7f96-441e-af7d-0e36a26b338e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Mesclar df_train com df_stores com base na coluna 'Store'\n",
        "df_train_stores = pd.merge(df_train, df_stores, on='Store', how='left')\n",
        "\n",
        "# 2. Mesclar o resultado com df_features com base em 'Store' e 'Date'\n",
        "df_train_merged = pd.merge(df_train_stores, df_features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "# 3. Verificar o resultado\n",
        "print(\"\\nShape de df_train_merged:\", df_train_merged.shape)\n",
        "print(\"\\nExibindo as 5 primeiras linhas do dataframe df_train_merged:\")\n",
        "display(df_train_merged.head())\n",
        "\n",
        "# 4. Verificar se há valores ausentes\n",
        "print(\"\\nValores ausentes por coluna:\")\n",
        "print(df_train_merged.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "IXrr51SllUBj",
        "outputId": "23cc5eef-3f1c-441a-e76a-7d890516e822"
      },
      "outputs": [],
      "source": [
        "# Para o conjunto de teste\n",
        "df_test_merged = pd.merge(df_test, df_stores, on='Store', how='left')\n",
        "df_test_merged = pd.merge(df_test_merged, df_features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "print(\"\\nExibindo as 5 primeiras linhas do dataframe df_test_merged:\")\n",
        "display(df_test_merged.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ8SxC7Ul164"
      },
      "source": [
        "# Limpeza de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA6lHtkwmTsM"
      },
      "source": [
        "Visualização dos dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8hprdpgl22z",
        "outputId": "e7a927ea-fb09-4fd9-b805-a97ab695db2d"
      },
      "outputs": [],
      "source": [
        "print(\"Informações sobre df_train_merged:\")\n",
        "df_train_merged.info()\n",
        "\n",
        "print(\"\\nInformações sobre df_test_merged:\")\n",
        "df_test_merged.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcX8qziBmRPm",
        "outputId": "d3577a3b-ea13-4d45-9c33-d52c5c735b65"
      },
      "outputs": [],
      "source": [
        "# Converter 'Date' para datetime\n",
        "df_train_merged['Date'] = pd.to_datetime(df_train_merged['Date'])\n",
        "df_test_merged['Date'] = pd.to_datetime(df_test_merged['Date'])\n",
        "\n",
        "df_train_merged.info()\n",
        "df_test_merged.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "Cmh88bwQmwkC",
        "outputId": "fc1f51e3-1af9-424b-93f1-d8a358411ea8"
      },
      "outputs": [],
      "source": [
        "print(\"--- Estatísticas Descritivas: df_train_merged ---\")\n",
        "display(df_train_merged.describe(include='all'))\n",
        "\n",
        "print(\"\\n--- Estatísticas Descritivas: df_test_merged ---\")\n",
        "display(df_test_merged.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc08Y00YozYQ"
      },
      "source": [
        "## Tratamento de Weekly_Sales Negativas e Colunas MarkDown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bd3s4Jgoxd6",
        "outputId": "b762fca4-ce8f-4fa2-b1f7-493bf2fdc42c"
      },
      "outputs": [],
      "source": [
        "num_vendas_negativas = (df_train_merged['Weekly_Sales'] < 0).sum()\n",
        "print(f\"Número de Weekly_Sales negativas: {num_vendas_negativas}\")\n",
        "if num_vendas_negativas > 0:\n",
        "    print(\"Substituindo Weekly_Sales negativas por 0...\")\n",
        "    df_train_merged.loc[df_train_merged['Weekly_Sales'] < 0, 'Weekly_Sales'] = 0\n",
        "    print(f\"Verificação - Novo mínimo de Weekly_Sales: {df_train_merged['Weekly_Sales'].min()}\")\n",
        "else:\n",
        "    print(\"Nenhuma Weekly_Sales negativa encontrada.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24qVaJw7pLtM",
        "outputId": "dc5bd356-3b30-4eae-e44b-f4773e0c0e84"
      },
      "outputs": [],
      "source": [
        "# 1. Tratamento de Weekly_Sales negativas em df_train_merged\n",
        "print(\"--- Tratando Weekly_Sales negativas em df_train_merged ---\")\n",
        "num_vendas_negativas = (df_train_merged['Weekly_Sales'] < 0).sum()\n",
        "if num_vendas_negativas > 0:\n",
        "    print(f\"Encontradas {num_vendas_negativas} Weekly_Sales negativas. Substituindo por 0...\")\n",
        "    df_train_merged.loc[df_train_merged['Weekly_Sales'] < 0, 'Weekly_Sales'] = 0\n",
        "    print(f\"Verificação - Novo mínimo de Weekly_Sales: {df_train_merged['Weekly_Sales'].min()}\")\n",
        "else:\n",
        "    print(\"Nenhuma Weekly_Sales negativa encontrada em df_train_merged.\")\n",
        "\n",
        "# 2. Tratamento de MarkDowns negativos\n",
        "\n",
        "markdown_cols_to_check = ['MarkDown2', 'MarkDown3']\n",
        "\n",
        "# Tratamento para df_train_merged\n",
        "print(\"\\n--- Tratando MarkDowns negativos em df_train_merged ---\")\n",
        "for col in markdown_cols_to_check:\n",
        "    if col in df_train_merged.columns:\n",
        "        num_neg_markdown_train = (df_train_merged[col] < 0).sum()\n",
        "        if num_neg_markdown_train > 0:\n",
        "            print(f\"Encontrados {num_neg_markdown_train} valores negativos em {col}. Substituindo por 0...\")\n",
        "            df_train_merged.loc[df_train_merged[col] < 0, col] = 0\n",
        "            print(f\"Verificação - Novo mínimo de {col}: {df_train_merged[col].min()}\")\n",
        "        else:\n",
        "            print(f\"Nenhum valor negativo encontrado em {col} no df_train_merged.\")\n",
        "    else:\n",
        "        print(f\"Coluna {col} não encontrada em df_train_merged.\")\n",
        "\n",
        "# Tratamento para df_test_merged\n",
        "print(\"\\n--- Tratando MarkDowns negativos em df_test_merged ---\")\n",
        "for col in markdown_cols_to_check:\n",
        "    if col in df_test_merged.columns:\n",
        "        num_neg_markdown_test = (df_test_merged[col] < 0).sum()\n",
        "        if num_neg_markdown_test > 0:\n",
        "            print(f\"Encontrados {num_neg_markdown_test} valores negativos em {col}. Substituindo por 0...\")\n",
        "            df_test_merged.loc[df_test_merged[col] < 0, col] = 0\n",
        "            print(f\"Verificação - Novo mínimo de {col}: {df_test_merged[col].min()}\")\n",
        "        else:\n",
        "            print(f\"Nenhum valor negativo encontrado em {col} no df_test_merged.\")\n",
        "    else:\n",
        "        print(f\"Coluna {col} não encontrada em df_test_merged.\")\n",
        "\n",
        "print(\"\\nTratamento de valores negativos concluído!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNfskG7voTmg",
        "outputId": "6de9178e-f9e7-498f-b722-3e0929784c7b"
      },
      "outputs": [],
      "source": [
        " # Validando valores negaticos:\n",
        "\n",
        "for i in range(1, 6):\n",
        "    col_name = f'MarkDown{i}'\n",
        "    print(f\"\\nAnálise de {col_name} em df_train_merged:\")\n",
        "    print(f\"Número de valores negativos: {(df_train_merged[col_name] < 0).sum()}\")\n",
        "    print(df_train_merged[df_train_merged[col_name] < 0][col_name].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5JQp0a7p9Iw"
      },
      "source": [
        "## Verificação de Duplicatas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCK1hT2zp98P",
        "outputId": "8f6fc69e-1c53-450a-eb3a-5bc08928cb0a"
      },
      "outputs": [],
      "source": [
        "# Para df_train_merged\n",
        "num_duplicatas_treino = df_train_merged.duplicated().sum()\n",
        "print(f\"\\nNúmero de linhas duplicadas em df_train_merged: {num_duplicatas_treino}\")\n",
        "if num_duplicatas_treino > 0:\n",
        "    print(f\"Removendo {num_duplicatas_treino} linhas duplicadas de df_train_merged...\")\n",
        "    df_train_merged.drop_duplicates(inplace=True)\n",
        "    df_train_merged.reset_index(drop=True, inplace=True) # Opcional: resetar o índice\n",
        "    print(f\"Novo shape de df_train_merged: {df_train_merged.shape}\")\n",
        "\n",
        "# Para df_test_merged\n",
        "num_duplicatas_teste = df_test_merged.duplicated().sum()\n",
        "print(f\"\\nNúmero de linhas duplicadas em df_test_merged: {num_duplicatas_teste}\")\n",
        "if num_duplicatas_teste > 0:\n",
        "    print(f\"Removendo {num_duplicatas_teste} linhas duplicadas de df_test_merged...\")\n",
        "    df_test_merged.drop_duplicates(inplace=True)\n",
        "    df_test_merged.reset_index(drop=True, inplace=True) # Opcional: resetar o índice\n",
        "    print(f\"Novo shape de df_test_merged: {df_test_merged.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2rU-qiIrAVW"
      },
      "source": [
        "## Tratamento de Valores Ausentes (NaNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMktjj5xrEfY"
      },
      "source": [
        "Verificando NaNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRPYyyi1rBPN",
        "outputId": "e7ccd6f0-f931-44c2-c803-8fe7b4528535"
      },
      "outputs": [],
      "source": [
        "print(\"--- Verificando NaNs em df_train_merged ANTES do tratamento ---\")\n",
        "print(df_train_merged.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Verificando NaNs em df_test_merged ANTES do tratamento ---\")\n",
        "print(df_test_merged.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOEZ2--FrVSp"
      },
      "source": [
        "Tratamento de NaNs nas colunas MarkDown1 a MarkDown5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f6tyjYLrUaf",
        "outputId": "c7b411e9-9699-4d44-8ed5-dcef421ed8f8"
      },
      "outputs": [],
      "source": [
        "# 1. Tratamento de NaNs nas colunas MarkDown1 a MarkDown5\n",
        "# Estratégia: Preencher NaNs com 0, pois indicam ausência de promoção.\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "print(\"\\n--- Tratando NaNs nas colunas MarkDown (preenchendo com 0) ---\")\n",
        "for col in markdown_cols:\n",
        "    if col in df_train_merged.columns:\n",
        "        df_train_merged[col] = df_train_merged[col].fillna(0)\n",
        "    if col in df_test_merged.columns:\n",
        "        df_test_merged[col] = df_test_merged[col].fillna(0)\n",
        "print(\"Preenchimento de NaNs nas colunas MarkDown concluído.\")\n",
        "\n",
        "# 2. Tratamento de NaNs nas colunas CPI e Unemployment\n",
        "# Estratégia: Preenchimento progressivo (ffill) e depois regressivo (bfill), agrupado por 'Store'.\n",
        "cols_to_ffill_bfill = ['CPI', 'Unemployment']\n",
        "\n",
        "print(\"\\n--- Tratando NaNs em CPI e Unemployment (ffill e bfill por Loja) ---\")\n",
        "\n",
        "# Para df_train_merged\n",
        "print(\"Tratando df_train_merged para CPI e Unemployment...\")\n",
        "for col in cols_to_ffill_bfill:\n",
        "    if col in df_train_merged.columns:\n",
        "        # Agrupa por 'Store', aplica ffill e depois bfill dentro de cada grupo\n",
        "        df_train_merged[col] = df_train_merged.groupby('Store')[col].ffill()\n",
        "        df_train_merged[col] = df_train_merged.groupby('Store')[col].bfill()\n",
        "    else:\n",
        "        print(f\"Coluna {col} não encontrada em df_train_merged.\")\n",
        "\n",
        "# Para df_test_merged\n",
        "print(\"Tratando df_test_merged para CPI e Unemployment...\")\n",
        "for col in cols_to_ffill_bfill:\n",
        "    if col in df_test_merged.columns:\n",
        "        # Agrupa por 'Store', aplica ffill e depois bfill dentro de cada grupo\n",
        "        df_test_merged[col] = df_test_merged.groupby('Store')[col].ffill()\n",
        "        df_test_merged[col] = df_test_merged.groupby('Store')[col].bfill()\n",
        "    else:\n",
        "        print(f\"Coluna {col} não encontrada em df_test_merged.\")\n",
        "print(\"Preenchimento de NaNs em CPI e Unemployment concluído.\")\n",
        "\n",
        "# 3. Verificação final de NaNs\n",
        "print(\"\\n--- Verificando NaNs em df_train_merged APÓS o tratamento ---\")\n",
        "print(df_train_merged.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Verificando NaNs em df_test_merged APÓS o tratamento ---\")\n",
        "print(df_test_merged.isnull().sum())\n",
        "\n",
        "print(\"\\nTratamento de valores ausentes (NaNs) concluído!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGhtBZZut1k8"
      },
      "source": [
        "## Transformação de Variáveis Categóricas (One-Hot Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw_X0knEuBWM"
      },
      "source": [
        "### Alteração de colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "oJjiYvARt1Sj",
        "outputId": "525f1eb7-82fe-493a-fd79-c077ae8b23ad"
      },
      "outputs": [],
      "source": [
        "print(\"--- Transformando a coluna 'Type' (One-Hot Encoding) ---\")\n",
        "\n",
        "# Aplicar em df_train_merged\n",
        "df_train_merged = pd.get_dummies(df_train_merged, columns=['Type'], prefix='Type')\n",
        "print(\"Coluna 'Type' transformada em df_train_merged.\")\n",
        "display(df_train_merged.head(2)) # Mostrar algumas linhas para ver as novas colunas\n",
        "\n",
        "# Aplicar em df_test_merged\n",
        "df_test_merged = pd.get_dummies(df_test_merged, columns=['Type'], prefix='Type')\n",
        "print(\"\\nColuna 'Type' transformada em df_test_merged.\")\n",
        "display(df_test_merged.head(2)) # Mostrar algumas linhas para ver as novas colunas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1eijM-quh-Z"
      },
      "source": [
        "Transformação da Variável Booleana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "7heSep_DuiSf",
        "outputId": "7fb6a99f-8e73-4647-c40b-4a7a2cc05ab2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Mesclar df_train com df_stores com base na coluna 'Store'\n",
        "df_train_stores = pd.merge(df_train, df_stores, on='Store', how='left')\n",
        "\n",
        "# 2. Mesclar o resultado com df_features com base em 'Store' e 'Date'\n",
        "df_train_merged = pd.merge(df_train_stores, df_features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "# 3. Verificar o resultado\n",
        "print(\"\\nShape de df_train_merged:\", df_train_merged.shape)\n",
        "print(\"\\nExibindo as 5 primeiras linhas do dataframe df_train_merged:\")\n",
        "display(df_train_merged.head())\n",
        "\n",
        "# 4. Verificar se há valores ausentes\n",
        "print(\"\\nValores ausentes por coluna:\")\n",
        "print(df_train_merged.isnull().sum())\n",
        "\n",
        "# 5. Tratar a coluna 'IsHoliday'\n",
        "print(\"\\n--- Tratando a coluna 'IsHoliday' ---\")\n",
        "\n",
        "# Verificar se 'IsHoliday_x' e 'IsHoliday_y' são consistentes (opcional, para debugging)\n",
        "if 'IsHoliday_x' in df_train_merged.columns and 'IsHoliday_y' in df_train_merged.columns:\n",
        "    discrepancias = df_train_merged[df_train_merged['IsHoliday_x'] != df_train_merged['IsHoliday_y']]\n",
        "    if not discrepancias.empty:\n",
        "        print(f\"ATENÇÃO: {len(discrepancias)} linhas com discrepâncias entre 'IsHoliday_x' e 'IsHoliday_y'.\")\n",
        "        print(\"Exemplo de discrepâncias:\")\n",
        "        display(discrepancias[['Date', 'IsHoliday_x', 'IsHoliday_y']].head())\n",
        "    # Usar 'IsHoliday_x' como a fonte principal\n",
        "    df_train_merged['IsHoliday'] = df_train_merged['IsHoliday_x']\n",
        "else:\n",
        "    print(\"Uma ou ambas as colunas 'IsHoliday_x' ou 'IsHoliday_y' não estão presentes.\")\n",
        "\n",
        "# 6. Transformar 'IsHoliday' para int (0/1)\n",
        "df_train_merged['IsHoliday'] = df_train_merged['IsHoliday'].astype(int)\n",
        "print(\"Coluna 'IsHoliday' transformada para int em df_train_merged.\")\n",
        "\n",
        "# 7. Repetir para df_test_merged\n",
        "df_test_stores = pd.merge(df_test, df_stores, on='Store', how='left')\n",
        "df_test_merged = pd.merge(df_test_stores, df_features, on=['Store', 'Date'], how='left')\n",
        "df_test_merged['IsHoliday'] = df_test_merged['IsHoliday_x'].astype(int)\n",
        "print(\"Coluna 'IsHoliday' transformada para int em df_test_merged.\")\n",
        "\n",
        "# 8. Exibir amostra para verificação\n",
        "print(\"\\nExibindo as 3 primeiras linhas de 'Date' e 'IsHoliday' em df_train_merged:\")\n",
        "display(df_train_merged[['Date', 'IsHoliday']].head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMkGjJT7u9Wb"
      },
      "source": [
        "# Engenharia de Features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBmhgp3J0C5z"
      },
      "source": [
        "## Criação de Features Temporais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "7Zus_g0QvBFS",
        "outputId": "63e17cf9-a3a8-4843-c88f-9bcee9d38095"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Verificar e converter 'Date' para datetime em df_train_merged\n",
        "print(\"--- Verificando e convertendo 'Date' em df_train_merged ---\")\n",
        "if 'Date' not in df_train_merged.columns:\n",
        "    print(\"ERRO: Coluna 'Date' não encontrada em df_train_merged.\")\n",
        "    print(\"Colunas disponíveis:\", df_train_merged.columns.tolist())\n",
        "else:\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df_train_merged['Date']):\n",
        "        df_train_merged['Date'] = pd.to_datetime(df_train_merged['Date'], errors='coerce')\n",
        "        print(\"Coluna 'Date' convertida para datetime em df_train_merged.\")\n",
        "    else:\n",
        "        print(\"Coluna 'Date' já está em formato datetime em df_train_merged.\")\n",
        "\n",
        "# 2. Verificar e converter 'Date' para datetime em df_test_merged\n",
        "print(\"\\n--- Verificando e convertendo 'Date' em df_test_merged ---\")\n",
        "if 'Date' not in df_test_merged.columns:\n",
        "    print(\"ERRO: Coluna 'Date' não encontrada em df_test_merged.\")\n",
        "    print(\"Colunas disponíveis:\", df_test_merged.columns.tolist())\n",
        "else:\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df_test_merged['Date']):\n",
        "        df_test_merged['Date'] = pd.to_datetime(df_test_merged['Date'], errors='coerce')\n",
        "        print(\"Coluna 'Date' convertida para datetime em df_test_merged.\")\n",
        "    else:\n",
        "        print(\"Coluna 'Date' já está em formato datetime em df_test_merged.\")\n",
        "\n",
        "# 3. Função para criar features temporais\n",
        "print(\"\\n--- Atualizando a função para criar mais Features Temporais ---\")\n",
        "\n",
        "def criar_features_temporais_v2(df):\n",
        "    df_copy = df.copy()\n",
        "    \n",
        "    # Features básicas\n",
        "    df_copy['Year'] = df_copy['Date'].dt.year\n",
        "    df_copy['Month'] = df_copy['Date'].dt.month\n",
        "    df_copy['Day'] = df_copy['Date'].dt.day\n",
        "    df_copy['WeekOfYear'] = df_copy['Date'].dt.isocalendar().week.astype(int)\n",
        "    df_copy['DayOfWeek'] = df_copy['Date'].dt.dayofweek  # Segunda=0, Domingo=6\n",
        "    df_copy['DayOfYear'] = df_copy['Date'].dt.dayofyear\n",
        "    \n",
        "    # Novas features\n",
        "    df_copy['Quarter'] = df_copy['Date'].dt.quarter\n",
        "    df_copy['IsMonthStart'] = df_copy['Date'].dt.is_month_start.astype(int)\n",
        "    df_copy['IsMonthEnd'] = df_copy['Date'].dt.is_month_end.astype(int)\n",
        "    df_copy['IsYearStart'] = df_copy['Date'].dt.is_year_start.astype(int)\n",
        "    df_copy['IsYearEnd'] = df_copy['Date'].dt.is_year_end.astype(int)\n",
        "    \n",
        "    # Semana do Mês\n",
        "    df_copy['WeekOfMonth'] = (df_copy['Date'].dt.day - 1) // 7 + 1\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "# 4. Aplicar a função em df_train_merged\n",
        "if 'Date' in df_train_merged.columns and pd.api.types.is_datetime64_any_dtype(df_train_merged['Date']):\n",
        "    df_train_merged = criar_features_temporais_v2(df_train_merged)\n",
        "    print(\"\\nNovas features temporais adicionadas a df_train_merged.\")\n",
        "    cols_to_display = ['Date', 'Year', 'Month', 'Day', 'WeekOfYear', 'DayOfWeek', 'DayOfYear', \n",
        "                       'Quarter', 'IsMonthStart', 'IsMonthEnd', 'IsYearStart', 'IsYearEnd', 'WeekOfMonth']\n",
        "    cols_to_display = [col for col in cols_to_display if col in df_train_merged.columns]\n",
        "    display(df_train_merged[cols_to_display].head())\n",
        "else:\n",
        "    print(\"ERRO: Coluna 'Date' não encontrada ou não é datetime em df_train_merged. Novas features temporais não criadas.\")\n",
        "\n",
        "# 5. Aplicar a função em df_test_merged\n",
        "if 'Date' in df_test_merged.columns and pd.api.types.is_datetime64_any_dtype(df_test_merged['Date']):\n",
        "    df_test_merged = criar_features_temporais_v2(df_test_merged)\n",
        "    print(\"\\nNovas features temporais adicionadas a df_test_merged.\")\n",
        "    cols_to_display_test = [col for col in cols_to_display if col in df_test_merged.columns]\n",
        "    display(df_test_merged[cols_to_display_test].head())\n",
        "else:\n",
        "    print(\"ERRO: Coluna 'Date' não encontrada ou não é datetime em df_test_merged. Novas features temporais não criadas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hzyM8VlgaiU"
      },
      "source": [
        "## Criação de Features de Lag (Defasagem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aJVRVfLlvBgP",
        "outputId": "440077e8-66e5-4040-e5b1-d47dc6a96dc8"
      },
      "outputs": [],
      "source": [
        "print(\"--- Criando Features de Lag (com ajuste para FutureWarning) ---\")\n",
        "\n",
        "# 1. Preparar dataframes temporários para a criação de lags\n",
        "# Selecionar colunas chave e a variável alvo (Weekly_Sales apenas do treino)\n",
        "df_train_temp_for_lag = df_train_merged[['Store', 'Dept', 'Date', 'Weekly_Sales']].copy()\n",
        "df_test_temp_for_lag = df_test_merged[['Store', 'Dept', 'Date']].copy()\n",
        "\n",
        "# Adicionar coluna Weekly_Sales com np.nan para o teste (para consistência de tipo float)\n",
        "df_test_temp_for_lag['Weekly_Sales'] = np.nan #\n",
        "\n",
        "# Adicionar um identificador para separar depois\n",
        "df_train_temp_for_lag['is_train'] = 1\n",
        "df_test_temp_for_lag['is_train'] = 0\n",
        "\n",
        "# 2. Concatenar os dataframes temporários\n",
        "# Este é o ponto onde o FutureWarning ocorria. O ajuste acima com np.nan deve ajudar.\n",
        "df_combined = pd.concat([df_train_temp_for_lag, df_test_temp_for_lag], ignore_index=True)\n",
        "\n",
        "# 3. Ordenar corretamente ANTES de aplicar o shift dentro dos grupos\n",
        "df_combined.sort_values(by=['Store', 'Dept', 'Date'], inplace=True)\n",
        "\n",
        "# 4. Definir os lags que queremos criar\n",
        "lags_to_create = [1, 2, 3, 4, 12, 26, 52] # Ex: 1 semana, 2 sem, ..., 1 ano\n",
        "\n",
        "for lag in lags_to_create:\n",
        "    # O groupby garante que o shift é feito dentro de cada série individual de Loja/Departamento\n",
        "    df_combined[f'Weekly_Sales_lag_{lag}'] = df_combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n",
        "\n",
        "print(f\"Features de lag {lags_to_create} criadas no dataframe combinado.\")\n",
        "\n",
        "# 5. Separar de volta em treino e teste\n",
        "df_train_with_lags = df_combined[df_combined['is_train'] == 1].copy()\n",
        "df_test_with_lags = df_combined[df_combined['is_train'] == 0].copy()\n",
        "\n",
        "# Remover colunas auxiliares\n",
        "df_train_with_lags.drop(columns=['is_train'], inplace=True)\n",
        "# Para o teste, remover também a coluna 'Weekly_Sales' que foi preenchida com np.nan\n",
        "df_test_with_lags.drop(columns=['is_train', 'Weekly_Sales'], inplace=True)\n",
        "\n",
        "# 6. Juntar as novas colunas de lag de volta aos dataframes originais\n",
        "# Merge para df_train_merged (usando as colunas originais para garantir a junção correta)\n",
        "df_train_merged = pd.merge(\n",
        "    df_train_merged,\n",
        "    df_train_with_lags, # Contém Store, Dept, Date, Weekly_Sales (original) e os lags\n",
        "    on=['Store', 'Dept', 'Date', 'Weekly_Sales'], # Chaves para garantir correspondência exata\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Merge para df_test_merged\n",
        "df_test_merged = pd.merge(\n",
        "    df_test_merged,\n",
        "    df_test_with_lags, # Contém Store, Dept, Date e os lags\n",
        "    on=['Store', 'Dept', 'Date'], # Chaves para o teste\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(\"Features de lag adicionadas a df_train_merged e df_test_merged.\")\n",
        "\n",
        "# 7. Verificar as novas colunas e os NaNs introduzidos pelos lags (opcional, mas recomendado)\n",
        "print(\"\\nExemplo de features de lag em df_train_merged (Loja 1, Dept 1):\")\n",
        "cols_to_show_train = ['Date', 'Weekly_Sales'] + [col for col in df_train_merged.columns if 'Weekly_Sales_lag' in col]\n",
        "# Filtrando para uma loja/departamento específico para facilitar a visualização dos lags\n",
        "display(df_train_merged[(df_train_merged['Store']==1) & (df_train_merged['Dept']==1)][cols_to_show_train].head(10))\n",
        "\n",
        "print(\"\\nExemplo de features de lag em df_test_merged (Loja 1, Dept 1):\")\n",
        "cols_to_show_test = ['Date'] + [col for col in df_test_merged.columns if 'Weekly_Sales_lag' in col]\n",
        "display(df_test_merged[(df_test_merged['Store']==1) & (df_test_merged['Dept']==1)][cols_to_show_test].head(10))\n",
        "\n",
        "print(\"\\nVerificando NaNs introduzidos pelos lags em df_train_merged:\")\n",
        "print(df_train_merged[[col for col in df_train_merged.columns if 'Weekly_Sales_lag' in col]].isnull().sum().sort_values(ascending=False))\n",
        "\n",
        "print(\"\\nVerificando NaNs introduzidos pelos lags em df_test_merged:\")\n",
        "print(df_test_merged[[col for col in df_test_merged.columns if 'Weekly_Sales_lag' in col]].isnull().sum().sort_values(ascending=False))\n",
        "\n",
        "print(\"\\nCriação de features de lag concluída!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Criação de Features de Janela Móvel (Rolling Window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Converter a coluna 'Date' para datetime, se ainda não estiver\n",
        "# df_train['Date'] = pd.to_datetime(df_train['Date'])\n",
        "# df_test['Date'] = pd.to_datetime(df_test['Date'])\n",
        "\n",
        "# Definir a data de corte para a divisão treino/validação\n",
        "train_end_date = '2012-07-31'\n",
        "val_start_date = '2012-08-01'\n",
        "\n",
        "# Dividir df_train em treino e validação com base na data\n",
        "df_train_subset = df_train[df_train['Date'] <= train_end_date].copy()\n",
        "df_val_subset = df_train[(df_train['Date'] >= val_start_date) & (df_train['Date'] <= '2012-10-26')].copy()\n",
        "\n",
        "print(f\"Treino: {df_train_subset['Date'].min()} a {df_train_subset['Date'].max()} ({len(df_train_subset)} linhas)\")\n",
        "print(f\"Validação: {df_val_subset['Date'].min()} a {df_val_subset['Date'].max()} ({len(df_val_subset)} linhas)\")\n",
        "print(f\"Teste: {df_test['Date'].min()} a {df_test['Date'].max()} ({len(df_test)} linhas)\")\n",
        "\n",
        "# --- Criando Features de Janela Móvel ---\n",
        "window_sizes = [4, 8, 12, 26, 52]\n",
        "stats_to_calculate = ['mean', 'median', 'sum', 'std']\n",
        "\n",
        "# 1. Calcular features para o conjunto de treino\n",
        "df_train_subset = df_train_subset.sort_values(['Store', 'Dept', 'Date'])\n",
        "for window in window_sizes:\n",
        "    for stat in stats_to_calculate:\n",
        "        new_col_name = f'Weekly_Sales_roll_{stat}_{window}'\n",
        "        df_train_subset[new_col_name] = df_train_subset.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "            lambda s: s.rolling(window=window, min_periods=1).agg(stat).shift(1)\n",
        "        )\n",
        "        print(f\"Criada feature no treino: {new_col_name}\")\n",
        "\n",
        "# 2. Calcular features para o conjunto de validação\n",
        "df_val_subset = df_val_subset.sort_values(['Store', 'Dept', 'Date'])\n",
        "for window in window_sizes:\n",
        "    for stat in stats_to_calculate:\n",
        "        new_col_name = f'Weekly_Sales_roll_{stat}_{window}'\n",
        "        # Concatenar dados de treino com validação para garantir continuidade temporal\n",
        "        df_temp = pd.concat([\n",
        "            df_train_subset[['Store', 'Dept', 'Date', 'Weekly_Sales']],\n",
        "            df_val_subset[['Store', 'Dept', 'Date']].assign(Weekly_Sales=np.nan)\n",
        "        ])\n",
        "        df_temp = df_temp.sort_values(['Store', 'Dept', 'Date'])\n",
        "        df_temp[new_col_name] = df_temp.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "            lambda s: s.rolling(window=window, min_periods=1).agg(stat).shift(1)\n",
        "        )\n",
        "        # Atribuir as features ao conjunto de validação\n",
        "        df_val_subset[new_col_name] = df_temp[df_temp['Date'] >= val_start_date][new_col_name].values\n",
        "        print(f\"Criada feature na validação: {new_col_name}\")\n",
        "\n",
        "# 3. Calcular features para o conjunto de teste\n",
        "df_test = df_test.sort_values(['Store', 'Dept', 'Date'])\n",
        "for window in window_sizes:\n",
        "    for stat in stats_to_calculate:\n",
        "        new_col_name = f'Weekly_Sales_roll_{stat}_{window}'\n",
        "        # Concatenar dados de treino com teste para garantir continuidade temporal\n",
        "        df_temp = pd.concat([\n",
        "            df_train[['Store', 'Dept', 'Date', 'Weekly_Sales']],\n",
        "            df_test[['Store', 'Dept', 'Date']].assign(Weekly_Sales=np.nan)\n",
        "        ])\n",
        "        df_temp = df_temp.sort_values(['Store', 'Dept', 'Date'])\n",
        "        df_temp[new_col_name] = df_temp.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "            lambda s: s.rolling(window=window, min_periods=1).agg(stat).shift(1)\n",
        "        )\n",
        "        # Atribuir as features ao conjunto de teste\n",
        "        df_test[new_col_name] = df_temp[df_temp['Date'] >= '2012-11-02'][new_col_name].values\n",
        "        print(f\"Criada feature no teste: {new_col_name}\")\n",
        "\n",
        "# 4. Verificar e tratar NaNs\n",
        "print(\"\\nVerificando NaNs no df_train_subset:\")\n",
        "print(df_train_subset[[col for col in df_train_subset.columns if 'Weekly_Sales_roll_' in col]].isnull().sum())\n",
        "print(\"\\nVerificando NaNs no df_val_subset:\")\n",
        "print(df_val_subset[[col for col in df_val_subset.columns if 'Weekly_Sales_roll_' in col]].isnull().sum())\n",
        "print(\"\\nVerificando NaNs no df_test:\")\n",
        "print(df_test[[col for col in df_test.columns if 'Weekly_Sales_roll_' in col]].isnull().sum())\n",
        "\n",
        "# Tratar NaNs (preencher com a média por grupo)\n",
        "for col in [col for col in df_train_subset.columns if 'Weekly_Sales_roll_' in col]:\n",
        "    df_train_subset[col] = df_train_subset.groupby(['Store', 'Dept'])[col].transform(lambda x: x.fillna(x.mean()))\n",
        "    df_val_subset[col] = df_val_subset.groupby(['Store', 'Dept'])[col].transform(lambda x: x.fillna(x.mean()))\n",
        "    df_test[col] = df_test.groupby(['Store', 'Dept'])[col].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "print(\"\\nCriação de features de janela móvel concluída!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTjelYS8u-Md"
      },
      "source": [
        "## Tratamento Final de NaNs nas Features Criadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjXW6_zrvAzp",
        "outputId": "30338946-65cf-4755-f4cb-2c618a2e2e1f"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# NaNs na validação e teste, para evitar data leakage.\n",
        "print(\"\\n--- Iniciando Tratamento de NaNs para Features de Lag e Janela Móvel ---\")\n",
        "\n",
        "# Identificar as colunas de janela móvel e lag em df_train_subset\n",
        "# (Assumindo que df_val_subset e df_test terão as mesmas colunas de features)\n",
        "if 'df_train_subset' in locals():\n",
        "    cols_roll = [col for col in df_train_subset.columns if 'Weekly_Sales_roll_' in col]\n",
        "    cols_lag = [col for col in df_train_subset.columns if 'Weekly_Sales_lag_' in col]\n",
        "    cols_to_treat = cols_roll + cols_lag\n",
        "else:\n",
        "    print(\"ERRO: df_train_subset não está definido. Interrompendo tratamento de NaNs.\")\n",
        "\n",
        "    cols_to_treat = [] # Evitar erro mais abaixo\n",
        "\n",
        "if cols_to_treat:\n",
        "    # Passo 1: Calcular as médias por grupo apenas no conjunto de treino (df_train_subset)\n",
        "    print(\"\\nCalculando médias por grupo no conjunto de treino para preenchimento...\")\n",
        "    group_means = {}\n",
        "    for col in cols_to_treat:\n",
        "        if col in df_train_subset.columns:\n",
        "            means = df_train_subset.groupby(['Store', 'Dept'])[col].mean()\n",
        "            group_means[col] = means\n",
        "        else:\n",
        "            print(f\"AVISO: Coluna {col} não encontrada em df_train_subset para cálculo de média.\")\n",
        "\n",
        "    # Passo 2: Preencher NaNs em df_train_subset usando as médias do próprio conjunto\n",
        "    print(\"\\nTratando NaNs em df_train_subset...\")\n",
        "    for col in cols_to_treat:\n",
        "        if col in df_train_subset.columns:\n",
        "            # Usar a média do grupo do próprio treino para preencher NaNs do treino\n",
        "            df_train_subset[f'{col}_filled_temp'] = df_train_subset.groupby(['Store', 'Dept'])[col].transform(lambda x: x.fillna(x.mean()))\n",
        "            df_train_subset[col] = df_train_subset[f'{col}_filled_temp'].fillna(0) # Fallback para 0 se a média do grupo for NaN\n",
        "            df_train_subset.drop(columns=[f'{col}_filled_temp'], inplace=True)\n",
        "    print(\"NaNs preenchidos em df_train_subset.\")\n",
        "\n",
        "    # Passo 3: Preencher NaNs em df_val_subset usando as médias calculadas do conjunto de treino\n",
        "    print(\"\\nTratando NaNs em df_val_subset...\")\n",
        "    if 'df_val_subset' in locals():\n",
        "        for col in cols_to_treat:\n",
        "            if col in df_val_subset.columns and col in group_means:\n",
        "                # Mapear as médias do treino para o conjunto de validação\n",
        "                mapped_means = df_val_subset.set_index(['Store', 'Dept']).index.map(group_means[col])\n",
        "                df_val_subset[col] = df_val_subset[col].fillna(pd.Series(mapped_means, index=df_val_subset.index))\n",
        "                df_val_subset[col] = df_val_subset[col].fillna(0) # Fallback para 0\n",
        "            elif col not in group_means and col in df_val_subset.columns:\n",
        "                 print(f\"AVISO: Não há média de treino para {col}. Preenchendo NaNs em df_val_subset com 0.\")\n",
        "                 df_val_subset[col] = df_val_subset[col].fillna(0)\n",
        "\n",
        "        print(\"NaNs preenchidos em df_val_subset.\")\n",
        "    else:\n",
        "        print(\"AVISO: df_val_subset não definido. Pule o tratamento de NaNs para validação.\")\n",
        "\n",
        "\n",
        "    # Passo 4: Preencher NaNs em df_test usando as médias calculadas do conjunto de treino\n",
        "    print(\"\\nTratando NaNs em df_test...\")\n",
        "    if 'df_test' in locals():\n",
        "        for col in cols_to_treat:\n",
        "            if col in df_test.columns and col in group_means:\n",
        "                # Mapear as médias do treino para o conjunto de teste\n",
        "                mapped_means = df_test.set_index(['Store', 'Dept']).index.map(group_means[col])\n",
        "                df_test[col] = df_test[col].fillna(pd.Series(mapped_means, index=df_test.index))\n",
        "                df_test[col] = df_test[col].fillna(0) # Fallback para 0\n",
        "            elif col not in group_means and col in df_test.columns:\n",
        "                print(f\"AVISO: Não há média de treino para {col}. Preenchendo NaNs em df_test com 0.\")\n",
        "                df_test[col] = df_test[col].fillna(0)\n",
        "        print(\"NaNs preenchidos em df_test.\")\n",
        "    else:\n",
        "        print(\"AVISO: df_test não definido. Pule o tratamento de NaNs para teste.\")\n",
        "\n",
        "\n",
        "    # Verificação final de NaNs\n",
        "    print(\"\\n--- Verificando NaNs APÓS o tratamento final ---\")\n",
        "    if 'df_train_subset' in locals():\n",
        "        print(\"\\nFeatures em df_train_subset:\")\n",
        "        print(df_train_subset[cols_to_treat].isnull().sum())\n",
        "    if 'df_val_subset' in locals():\n",
        "        print(\"\\nFeatures em df_val_subset:\")\n",
        "        print(df_val_subset[cols_to_treat].isnull().sum())\n",
        "    if 'df_test' in locals():\n",
        "        print(\"\\nFeatures em df_test:\")\n",
        "        print(df_test[cols_to_treat].isnull().sum())\n",
        "    print(\"\\nTratamento final de NaNs concluído!\")\n",
        "else:\n",
        "    print(\"Nenhuma coluna de lag ou rolling para tratar NaNs foi identificada em df_train_subset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Tratando NaNs Finais nas Features de Janela Móvel e Lag (usando médias do treino para evitar leakage) ---\n",
        "print(\"--- Tratando NaNs Finais nas Features de Janela Móvel e Lag (usando médias do treino para evitar leakage) ---\")\n",
        "\n",
        "\n",
        "# Identificar as colunas de janela móvel e lag\n",
        "cols_roll = [col for col in df_train_subset.columns if 'Weekly_Sales_roll_' in col]\n",
        "cols_lag = [col for col in df_train_subset.columns if 'Weekly_Sales_lag_' in col]\n",
        "cols_to_treat = cols_roll + cols_lag # Lista combinada de colunas a serem tratadas\n",
        "\n",
        "# Passo 1: Calcular as médias por grupo apenas no conjunto de treino (df_train_subset)\n",
        "print(\"\\nCalculando médias por grupo no conjunto de treino...\")\n",
        "group_means = {}\n",
        "for col in cols_to_treat: # Usar a lista combinada\n",
        "    if col in df_train_subset.columns:\n",
        "        means = df_train_subset.groupby(['Store', 'Dept'])[col].mean()\n",
        "        group_means[col] = means\n",
        "\n",
        "# Passo 2: Preencher NaNs em df_train_subset usando as médias do próprio conjunto e 0 como fallback\n",
        "print(\"\\nTratando NaNs em df_train_subset...\")\n",
        "for col in cols_to_treat:\n",
        "    if col in df_train_subset.columns:\n",
        "        # Cria uma coluna temporária com as médias do grupo para preenchimento\n",
        "        # Isso evita o warning de SettingWithCopyWarning em algumas versões do pandas com transform\n",
        "        df_train_subset[f'{col}_group_mean_fill'] = df_train_subset.groupby(['Store', 'Dept'])[col].transform(lambda x: x.fillna(x.mean()))\n",
        "        # Preenche a coluna original usando a coluna temporária\n",
        "        df_train_subset[col] = df_train_subset[f'{col}_group_mean_fill']\n",
        "        # Preenche quaisquer NaNs restantes (se a média do grupo também era NaN) com 0\n",
        "        df_train_subset[col] = df_train_subset[col].fillna(0)\n",
        "        # Remove a coluna temporária\n",
        "        df_train_subset.drop(columns=[f'{col}_group_mean_fill'], inplace=True)\n",
        "print(\"NaNs preenchidos em df_train_subset.\")\n",
        "\n",
        "# Passo 3: Preencher NaNs em df_val_subset usando as médias calculadas do conjunto de treino\n",
        "print(\"\\nTratando NaNs em df_val_subset...\")\n",
        "for col in cols_to_treat:\n",
        "    if col in df_val_subset.columns and col in group_means:\n",
        "        # Mapear as médias do treino (group_means) para o conjunto de validação\n",
        "        # Criar uma série de médias alinhada com o índice de df_val_subset\n",
        "        mapped_means_val = df_val_subset.set_index(['Store', 'Dept']).index.map(group_means[col])\n",
        "        # Atribuir ao índice correto para evitar desalinhamento ao usar fillna com uma Series\n",
        "        mapped_means_val_series = pd.Series(mapped_means_val, index=df_val_subset.index)\n",
        "        \n",
        "        df_val_subset[col] = df_val_subset[col].fillna(mapped_means_val_series)\n",
        "        df_val_subset[col] = df_val_subset[col].fillna(0) # Fallback para 0\n",
        "    elif col in df_val_subset.columns: # Se a coluna existe mas não há média do treino para ela\n",
        "        print(f\"AVISO: Não há média de treino para {col} (ou coluna não em group_means). Preenchendo NaNs em df_val_subset com 0.\")\n",
        "        df_val_subset[col] = df_val_subset[col].fillna(0)\n",
        "print(\"NaNs preenchidos em df_val_subset.\")\n",
        "\n",
        "# Passo 4: Preencher NaNs em df_test usando as médias calculadas do conjunto de treino\n",
        "print(\"\\nTratando NaNs em df_test...\")\n",
        "for col in cols_to_treat:\n",
        "    if col in df_test.columns and col in group_means:\n",
        "        # Mapear as médias do treino (group_means) para o conjunto de teste\n",
        "        mapped_means_test = df_test.set_index(['Store', 'Dept']).index.map(group_means[col])\n",
        "        mapped_means_test_series = pd.Series(mapped_means_test, index=df_test.index)\n",
        "        \n",
        "        df_test[col] = df_test[col].fillna(mapped_means_test_series)\n",
        "        df_test[col] = df_test[col].fillna(0) # Fallback para 0\n",
        "    elif col in df_test.columns: # Se a coluna existe mas não há média do treino para ela\n",
        "        print(f\"AVISO: Não há média de treino para {col} (ou coluna não em group_means). Preenchendo NaNs em df_test com 0.\")\n",
        "        df_test[col] = df_test[col].fillna(0)\n",
        "print(\"NaNs preenchidos em df_test.\")\n",
        "\n",
        "# Verificação final de NaNs\n",
        "print(\"\\n--- Verificando NaNs APÓS o tratamento final (V11) ---\")\n",
        "if 'df_train_subset' in locals() and cols_to_treat:\n",
        "    print(\"\\nFeatures em df_train_subset:\")\n",
        "    print(df_train_subset[cols_to_treat].isnull().sum())\n",
        "if 'df_val_subset' in locals() and cols_to_treat:\n",
        "    print(\"\\nFeatures em df_val_subset:\")\n",
        "    print(df_val_subset[cols_to_treat].isnull().sum())\n",
        "if 'df_test' in locals() and cols_to_treat:\n",
        "    print(\"\\nFeatures em df_test:\")\n",
        "    print(df_test[cols_to_treat].isnull().sum())\n",
        "\n",
        "print(\"\\nTratamento final de NaNs nas features de janela móvel e lag concluído (V11 - sem risco de leakage)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTDYAfLy1kqH"
      },
      "source": [
        "# Modelagem e Avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGHIfUc7zofu"
      },
      "source": [
        "## Divisão dos Dados (Definição das Variáveis (X e y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oisijq9VzpsJ",
        "outputId": "f67ad59e-b08d-4a80-b0f8-b2e9b9c6a67f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Garantir que a coluna 'Date' esteja no formato datetime\n",
        "df_train_merged['Date'] = pd.to_datetime(df_train_merged['Date'])\n",
        "df_test_merged['Date'] = pd.to_datetime(df_test_merged['Date'])\n",
        "\n",
        "# 2. Divisão dos dados em treino e validação a partir de df_train_merged\n",
        "# Ordenar por data para garantir divisão temporal\n",
        "df_train_merged = df_train_merged.sort_values('Date')\n",
        "\n",
        "# Definir o ponto de corte para 80% dos dados\n",
        "train_size = int(len(df_train_merged) * 0.8)\n",
        "df_treino = df_train_merged.iloc[:train_size].copy()\n",
        "df_validacao = df_train_merged.iloc[train_size:].copy()\n",
        "\n",
        "# 3. Definir a variável alvo (y) para treino e validação\n",
        "y_treino = df_treino['Weekly_Sales']\n",
        "y_validacao = df_validacao['Weekly_Sales']\n",
        "\n",
        "# 4. Definir as colunas de features\n",
        "# Excluir 'Weekly_Sales' (alvo), mas manter 'Date' para gráficos futuros\n",
        "colunas_features = [col for col in df_treino.columns if col != 'Weekly_Sales']\n",
        "\n",
        "# Garantir que as mesmas colunas estejam em todos os conjuntos\n",
        "X_treino = df_treino[colunas_features].copy()\n",
        "X_validacao = df_validacao[colunas_features].copy()\n",
        "X_teste = df_test_merged[colunas_features].copy()\n",
        "\n",
        "# 5. Criar cópias de 'Date' para uso futuro em gráficos\n",
        "date_treino = X_treino['Date'].copy()\n",
        "date_validacao = X_validacao['Date'].copy()\n",
        "date_teste = X_teste['Date'].copy()\n",
        "\n",
        "# 6. Remover 'Date' das features para modelagem\n",
        "colunas_features_modelagem = [col for col in colunas_features if col != 'Date']\n",
        "X_treino = X_treino[colunas_features_modelagem]\n",
        "X_validacao = X_validacao[colunas_features_modelagem]\n",
        "X_teste = X_teste[colunas_features_modelagem]\n",
        "\n",
        "# 7. Verificar os shapes\n",
        "print(\"--- Shapes das Variáveis Definidas ---\")\n",
        "print(f\"Shape de X_treino: {X_treino.shape}\")\n",
        "print(f\"Shape de y_treino: {y_treino.shape}\")\n",
        "print(f\"Shape de date_treino: {date_treino.shape}\")\n",
        "print(f\"Shape de X_validacao: {X_validacao.shape}\")\n",
        "print(f\"Shape de y_validacao: {y_validacao.shape}\")\n",
        "print(f\"Shape de date_validacao: {date_validacao.shape}\")\n",
        "print(f\"Shape de X_teste: {X_teste.shape}\")\n",
        "print(f\"Shape de date_teste: {date_teste.shape}\")\n",
        "\n",
        "# 8. Verificar as colunas\n",
        "print(\"\\n--- Colunas em X_treino ---\")\n",
        "print(X_treino.columns.tolist())\n",
        "\n",
        "# 9. Verificar consistência das colunas\n",
        "if list(X_treino.columns) == list(X_validacao.columns) == list(X_teste.columns):\n",
        "    print(\"\\nAs colunas de features estão consistentes entre os conjuntos.\")\n",
        "else:\n",
        "    print(\"\\nATENÇÃO: As colunas de features NÃO estão consistentes entre os conjuntos!\")\n",
        "    print(\"Colunas em X_treino:\", X_treino.columns.tolist())\n",
        "    print(\"Colunas em X_validacao:\", X_validacao.columns.tolist())\n",
        "    print(\"Colunas em X_teste:\", X_teste.columns.tolist())\n",
        "\n",
        "# 10. Verificar intervalos de tempo\n",
        "print(\"\\n--- Intervalos de Tempo ---\")\n",
        "print(f\"Treino - Data mínima: {date_treino.min()}, Data máxima: {date_treino.max()}\")\n",
        "print(f\"Validação - Data mínima: {date_validacao.min()}, Data máxima: {date_validacao.max()}\")\n",
        "print(f\"Teste - Data mínima: {date_teste.min()}, Data máxima: {date_teste.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIQJeiX22Mor"
      },
      "source": [
        "#  Treinamento e Avaliação de Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparação dos dados para treino do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# # (Assumindo que df_train_subset, df_val_subset, e df_test estão definidos a partir de um carregamento anterior)\n",
        "# # Exemplo: Carregamento de dados (substitua pelo seu método de carregamento real)\n",
        "# # df_train_subset = pd.read_csv('train_subset.csv')\n",
        "# # df_val_subset = pd.read_csv('val_subset.csv')\n",
        "\n",
        "# print(\"\\n--- Preparando Dados para Treinamento (X e y) ---\")\n",
        "\n",
        "# if 'df_train_subset' in locals() and 'df_val_subset' in locals():\n",
        "#     # Garantir que 'Date' está em formato datetime\n",
        "#     df_train_subset['Date'] = pd.to_datetime(df_train_subset['Date'])\n",
        "#     df_val_subset['Date'] = pd.to_datetime(df_val_subset['Date'])\n",
        "\n",
        "#     # Criar colunas temporais derivadas\n",
        "#     df_train_subset['Year'] = df_train_subset['Date'].dt.year\n",
        "#     df_train_subset['Month'] = df_train_subset['Date'].dt.month\n",
        "#     df_train_subset['WeekOfYear'] = df_train_subset['Date'].dt.isocalendar().week\n",
        "#     df_train_subset['DayOfYear'] = df_train_subset['Date'].dt.dayofyear\n",
        "#     df_train_subset['Quarter'] = df_train_subset['Date'].dt.quarter\n",
        "\n",
        "#     df_val_subset['Year'] = df_val_subset['Date'].dt.year\n",
        "#     df_val_subset['Month'] = df_val_subset['Date'].dt.month\n",
        "#     df_val_subset['WeekOfYear'] = df_val_subset['Date'].dt.isocalendar().week\n",
        "#     df_val_subset['DayOfYear'] = df_val_subset['Date'].dt.dayofyear\n",
        "#     df_val_subset['Quarter'] = df_val_subset['Date'].dt.quarter\n",
        "\n",
        "#     # Definir as features, excluindo colunas indesejadas\n",
        "#     features = [col for col in df_train_subset.columns \n",
        "#                 if col not in ['Store', 'Dept', 'Weekly_Sales', 'Date']\n",
        "#                 and not pd.api.types.is_datetime64_any_dtype(df_train_subset[col])]\n",
        "\n",
        "#     # Garantir que colunas temporais sejam incluídas\n",
        "#     colunas_temporais = ['Year', 'Month', 'WeekOfYear', 'DayOfYear', 'Quarter']\n",
        "#     features.extend([col for col in colunas_temporais if col in df_train_subset.columns and col not in features])\n",
        "\n",
        "#     X_treino = df_train_subset[features]\n",
        "#     y_treino = df_train_subset['Weekly_Sales']\n",
        "#     X_validacao = df_val_subset[features]\n",
        "#     y_validacao = df_val_subset['Weekly_Sales']\n",
        "\n",
        "#     if 'df_test' in locals():\n",
        "#         df_test['Date'] = pd.to_datetime(df_test['Date'])\n",
        "#         df_test['Year'] = df_test['Date'].dt.year\n",
        "#         df_test['Month'] = df_test['Date'].dt.month\n",
        "#         df_test['WeekOfYear'] = df_test['Date'].dt.isocalendar().week\n",
        "#         df_test['DayOfYear'] = df_test['Date'].dt.dayofyear\n",
        "#         df_test['Quarter'] = df_test['Date'].dt.quarter\n",
        "#         X_teste = df_test[features]\n",
        "#         print(f\"Shape de X_teste: {X_teste.shape}\")\n",
        "#     else:\n",
        "#         print(\"AVISO: df_test não definido. X_teste não foi criado.\")\n",
        "\n",
        "#     print(f\"Shape de X_treino: {X_treino.shape}, Shape de y_treino: {y_treino.shape}\")\n",
        "#     print(f\"Shape de X_validacao: {X_validacao.shape}, Shape de y_validacao: {y_validacao.shape}\")\n",
        "#     print(f\"\\nPrimeiras 5 features selecionadas: {features[:5]}\")\n",
        "#     print(f\"Total de features selecionadas: {len(features)}\")\n",
        "#     print(\"Colunas em X_validacao:\", X_validacao.columns.tolist())\n",
        "# else:\n",
        "#     print(\"ERRO: df_train_subset ou df_val_subset não estão definidos. Interrompendo.\")\n",
        "#     X_treino, y_treino, X_validacao, y_validacao = [None] * 4\n",
        "\n",
        "# # Treinar o modelo de regressão linear\n",
        "# print(\"\\n--- Treinando o Modelo de Regressão Linear ---\")\n",
        "# if X_treino is not None and y_treino is not None:\n",
        "#     modelo_lr = LinearRegression()\n",
        "#     modelo_lr.fit(X_treino, y_treino)\n",
        "\n",
        "#     # Gerar previsões\n",
        "#     y_pred_lr = modelo_lr.predict(X_validacao)\n",
        "#     print(\"Modelo treinado e previsões geradas com sucesso.\")\n",
        "# else:\n",
        "#     print(\"ERRO: X_treino ou y_treino não definidos. Não é possível treinar o modelo.\")\n",
        "\n",
        "# # Avaliar o modelo\n",
        "# print(\"\\n--- Avaliando o Modelo de Regressão Linear ---\")\n",
        "# if y_validacao is not None and y_pred_lr is not None:\n",
        "#     # Calcular métricas\n",
        "#     r2 = r2_score(y_validacao, y_pred_lr)\n",
        "#     mae = mean_absolute_error(y_validacao, y_pred_lr)\n",
        "#     mse = mean_squared_error(y_validacao, y_pred_lr)\n",
        "#     rmse = np.sqrt(mse)  # Calcular RMSE manualmente\n",
        "#     print(f\"Métricas de Avaliação - Regressão Linear:\")\n",
        "#     print(f\"R²: {r2:.4f}\")\n",
        "#     print(f\"MAE: {mae:.2f}\")\n",
        "#     print(f\"RMSE: {rmse:.2f}\")\n",
        "# else:\n",
        "#     print(\"ERRO: y_validacao ou y_pred_lr não definidos. Não é possível avaliar o modelo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnóstico de dados antes do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_treino is not None: # Prosseguir apenas se X_treino foi definido\n",
        "    print(\"\\n--- Diagnóstico de Dados Antes do Treinamento ---\")\n",
        "    print(\"\\nTipos de dados em X_treino (Top 5):\")\n",
        "    print(X_treino.dtypes.head())\n",
        "    print(\"\\nVerificando NaN em X_treino (Soma Total):\")\n",
        "    print(X_treino.isnull().sum().sum()) # Mostra a soma total de NaNs\n",
        "    print(\"\\nVerificando NaN em y_treino:\")\n",
        "    print(y_treino.isnull().sum())\n",
        "    print(\"\\nVerificando NaN em X_validacao (Soma Total):\")\n",
        "    print(X_validacao.isnull().sum().sum())\n",
        "    print(\"\\nVerificando NaN em y_validacao:\")\n",
        "    print(y_validacao.isnull().sum())\n",
        "    \n",
        "    print(\"\\nVerificando valores infinitos em X_treino (Soma Total):\")\n",
        "    print(np.isinf(X_treino.select_dtypes(include=np.number)).sum().sum()) # Apenas em colunas numéricas\n",
        "    print(\"\\nVerificando valores infinitos em y_treino:\")\n",
        "    print(np.isinf(y_treino).sum())\n",
        "    print(\"\\nVerificando valores infinitos em X_validacao (Soma Total):\")\n",
        "    print(np.isinf(X_validacao.select_dtypes(include=np.number)).sum().sum())\n",
        "    print(\"\\nVerificando valores infinitos em y_validacao:\")\n",
        "    print(np.isinf(y_validacao).sum())\n",
        "    print(\"\\nDiagnóstico concluído.\")\n",
        "else:\n",
        "    print(\"\\nDiagnóstico de dados não pode ser realizado pois X_treino não foi definido.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eom3NXx92RWg"
      },
      "source": [
        "## Treinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABKmt0Fb2Toe",
        "outputId": "89cf5897-46fe-47c3-ec05-740979a2a31d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import psutil  # Para monitorar uso de memória (opcional)\n",
        "\n",
        "print(\"\\n--- Iniciando Comparação de Modelos (Treino e Validação) ---\")\n",
        "\n",
        "# Função para monitorar uso de memória (opcional)\n",
        "def check_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_info = process.memory_info()\n",
        "    print(f\"Uso de memória atual: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Recriar X_treino, y_treino, X_validacao e y_validacao se necessário\n",
        "if 'X_treino' not in locals() or 'y_treino' not in locals() or 'X_validacao' not in locals() or 'y_validacao' not in locals():\n",
        "    print(\"\\nAVISO: X_treino, y_treino, X_validacao ou y_validacao não estão definidos. Recriando a partir de df_train_merged...\")\n",
        "    if 'df_train_merged' not in locals():\n",
        "        print(\"ERRO: df_train_merged não está definido. Não é possível recriar os dados.\")\n",
        "    else:\n",
        "        # Divisão temporal 80/20\n",
        "        df_train_merged = df_train_merged.sort_values('Date')\n",
        "        train_size = int(len(df_train_merged) * 0.8)\n",
        "        df_treino = df_train_merged.iloc[:train_size].copy()\n",
        "        df_validacao = df_train_merged.iloc[train_size:].copy()\n",
        "        y_treino = df_treino['Weekly_Sales'].copy()\n",
        "        y_validacao = df_validacao['Weekly_Sales'].copy()\n",
        "        colunas_a_excluir = ['Weekly_Sales', 'Date']\n",
        "        colunas_features = [col for col in df_treino.columns if col not in colunas_a_excluir]\n",
        "        colunas_features = [f for f in colunas_features if not pd.api.types.is_datetime64_any_dtype(df_treino[f])]\n",
        "        X_treino = df_treino[colunas_features].copy()\n",
        "        X_validacao = df_validacao[colunas_features].copy()\n",
        "        print(f\"Dados recriados com sucesso. Shape de X_treino: {X_treino.shape}, Shape de X_validacao: {X_validacao.shape}\")\n",
        "\n",
        "# Tratamento de NaNs e Infinitos antes de qualquer manipulação\n",
        "if 'X_treino' in locals() and 'X_validacao' in locals():\n",
        "    for df_X, nome_df in zip([X_treino, X_validacao], ['X_treino', 'X_validacao']):\n",
        "        print(f\"\\n--- Verificando e Tratando NaNs e Infinitos em {nome_df} ---\")\n",
        "        # Verificar NaNs\n",
        "        nan_check = df_X.isnull().sum()\n",
        "        colunas_com_nan = nan_check[nan_check > 0]\n",
        "        if not colunas_com_nan.empty:\n",
        "            print(f\"Colunas com NaNs em {nome_df} antes do preenchimento:\")\n",
        "            print(colunas_com_nan)\n",
        "            print(f\"Preenchendo NaNs em {nome_df}...\")\n",
        "            cpi_fill_value = X_treino['CPI'].median() if 'CPI' in X_treino.columns else 0\n",
        "            unemp_fill_value = X_treino['Unemployment'].median() if 'Unemployment' in X_treino.columns else 0\n",
        "            for col_nan in colunas_com_nan.index:\n",
        "                if col_nan == 'CPI':\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(cpi_fill_value)\n",
        "                elif col_nan == 'Unemployment':\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(unemp_fill_value)\n",
        "                else:\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(0)\n",
        "            print(f\"NaNs preenchidos em {nome_df}. Nova verificação:\")\n",
        "            print(df_X.isnull().sum()[df_X.isnull().sum() > 0])\n",
        "        else:\n",
        "            print(f\"Nenhum NaN encontrado em {nome_df}.\")\n",
        "        \n",
        "        # Verificar infinitos sem criar cópias desnecessárias\n",
        "        numeric_cols = df_X.select_dtypes(include=np.number).columns\n",
        "        inf_check = pd.Series(0, index=numeric_cols)  # Inicializa com zeros\n",
        "        for col in numeric_cols:\n",
        "            inf_check[col] = np.isinf(df_X[col]).sum()\n",
        "        colunas_com_inf = inf_check[inf_check > 0]\n",
        "        if not colunas_com_inf.empty:\n",
        "            print(f\"Colunas com infinitos em {nome_df}:\")\n",
        "            print(colunas_com_inf)\n",
        "            print(f\"Substituindo infinitos por NaN e preenchendo com 0 em {nome_df}...\")\n",
        "            df_X[numeric_cols] = df_X[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
        "            df_X[numeric_cols] = df_X[numeric_cols].fillna(0)\n",
        "            print(f\"Infinitos tratados em {nome_df}.\")\n",
        "\n",
        "# Codificação da coluna 'Type' após tratamento de NaNs\n",
        "if 'X_treino' in locals() and 'X_validacao' in locals():\n",
        "    print(\"\\n--- Codificando a Coluna 'Type' ---\")\n",
        "    type_mapping = {'A': 3, 'B': 2, 'C': 1}\n",
        "    if 'Type' in X_treino.columns:\n",
        "        try:\n",
        "            X_treino['Type'] = X_treino['Type'].map(type_mapping).astype('int32')\n",
        "            X_validacao['Type'] = X_validacao['Type'].map(type_mapping).astype('int32')\n",
        "            print(\"Valores únicos em 'Type' após mapeamento (X_treino):\", X_treino['Type'].unique())\n",
        "            print(\"Valores únicos em 'Type' após mapeamento (X_validacao):\", X_validacao['Type'].unique())\n",
        "        except Exception as e:\n",
        "            print(f\"ERRO ao codificar 'Type': {e}\")\n",
        "    else:\n",
        "        print(\"AVISO: Coluna 'Type' não encontrada em X_treino ou X_validacao.\")\n",
        "\n",
        "    # Verificar se há outras colunas categóricas\n",
        "    print(\"\\n--- Verificando Tipos de Dados para Identificar Colunas Categóricas ---\")\n",
        "    colunas_nao_numericas = X_treino.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    if colunas_nao_numericas:\n",
        "        print(f\"Colunas não numéricas encontradas em X_treino: {colunas_nao_numericas}\")\n",
        "        print(\"ERRO: Existem colunas categóricas que precisam ser codificadas antes de prosseguir.\")\n",
        "        X_treino_scaled, X_validacao_scaled = None, None\n",
        "    else:\n",
        "        print(\"Nenhuma coluna categórica adicional encontrada.\")\n",
        "\n",
        "# Diagnóstico de Dados (após tratamento e codificação)\n",
        "if X_treino is not None:\n",
        "    print(\"\\n--- Diagnóstico de Dados Após Tratamento ---\")\n",
        "    print(\"\\nTipos de dados em X_treino (Top 5):\")\n",
        "    print(X_treino.dtypes.head())\n",
        "    print(\"\\nVerificando NaN em X_treino (Soma Total):\")\n",
        "    print(X_treino.isnull().sum().sum())\n",
        "    print(\"\\nVerificando NaN em y_treino:\")\n",
        "    print(y_treino.isnull().sum())\n",
        "    print(\"\\nVerificando NaN em X_validacao (Soma Total):\")\n",
        "    print(X_validacao.isnull().sum().sum())\n",
        "    print(\"\\nVerificando NaN em y_validacao:\")\n",
        "    print(y_validacao.isnull().sum())\n",
        "    print(\"\\nVerificando valores infinitos em X_treino (Soma Total):\")\n",
        "    print(np.isinf(X_treino.select_dtypes(include=np.number)).sum().sum())\n",
        "    print(\"\\nVerificando valores infinitos em y_treino:\")\n",
        "    print(np.isinf(y_treino).sum())\n",
        "    print(\"\\nVerificando valores infinitos em X_validacao (Soma Total):\")\n",
        "    print(np.isinf(X_validacao.select_dtypes(include=np.number)).sum().sum())\n",
        "    print(\"\\nVerificando valores infinitos em y_validacao:\")\n",
        "    print(np.isinf(y_validacao).sum())\n",
        "    print(\"\\nDiagnóstico concluído.\")\n",
        "\n",
        "# Converter para tipos de dados otimizados para reduzir uso de memória\n",
        "if 'X_treino' in locals() and 'X_validacao' in locals() and colunas_nao_numericas == []:\n",
        "    print(\"\\n--- Otimizando Tipos de Dados para Reduzir Uso de Memória ---\")\n",
        "    for df_X, nome_df in zip([X_treino, X_validacao], ['X_treino', 'X_validacao']):\n",
        "        for col in df_X.columns:\n",
        "            if df_X[col].dtype == 'int64':\n",
        "                df_X[col] = df_X[col].astype('int32')\n",
        "            elif df_X[col].dtype == 'float64':\n",
        "                df_X[col] = df_X[col].astype('float32')\n",
        "            elif df_X[col].dtype == 'bool':\n",
        "                df_X[col] = df_X[col].astype('bool')  # Já é otimizado, mas para consistência\n",
        "        print(f\"Tipos de dados otimizados em {nome_df}:\")\n",
        "        print(df_X.dtypes.head())\n",
        "\n",
        "# Escalonar os dados\n",
        "if 'X_treino' in locals() and 'X_validacao' in locals() and colunas_nao_numericas == []:\n",
        "    check_memory_usage()  # Monitorar uso de memória antes do escalonamento\n",
        "    if 'scaler' not in locals():\n",
        "        print(\"\\nAVISO: Scaler não encontrado. Criando um novo scaler a partir de X_treino...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_treino_scaled = scaler.fit_transform(X_treino.to_numpy().astype(np.float32))\n",
        "        X_validacao_scaled = scaler.transform(X_validacao.to_numpy().astype(np.float32))\n",
        "    else:\n",
        "        print(\"\\nReutilizando scaler existente...\")\n",
        "        X_treino_scaled = scaler.transform(X_treino.to_numpy().astype(np.float32))\n",
        "        X_validacao_scaled = scaler.transform(X_validacao.to_numpy().astype(np.float32))\n",
        "    print(\"Escalonamento concluído.\")\n",
        "    check_memory_usage()  # Monitorar uso de memória após o escalonamento\n",
        "else:\n",
        "    print(\"\\nERRO: Dados de treino ou validação não estão definidos corretamente ou contêm colunas categóricas.\")\n",
        "    X_treino_scaled, X_validacao_scaled = None, None\n",
        "\n",
        "# Treinamento e Avaliação dos Modelos\n",
        "if X_treino_scaled is not None and y_treino is not None and X_validacao_scaled is not None and y_validacao is not None:\n",
        "    resultados_modelos = {}\n",
        "\n",
        "    # --- Modelo 1: Regressão Linear (Baseline) ---\n",
        "    print(\"\\nTreinando Modelo: Regressão Linear...\")\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_treino_scaled, y_treino)\n",
        "    y_pred_lr = lr_model.predict(X_validacao_scaled)\n",
        "    mae_lr = mean_absolute_error(y_validacao, y_pred_lr)\n",
        "    rmse_lr = np.sqrt(mean_squared_error(y_validacao, y_pred_lr))\n",
        "    r2_lr = r2_score(y_validacao, y_pred_lr)\n",
        "    print(f\"Regressão Linear - MAE: {mae_lr:.2f}, RMSE: {rmse_lr:.2f}, R²: {r2_lr:.4f}\")\n",
        "    resultados_modelos['Regressão Linear'] = {'MAE': mae_lr, 'RMSE': rmse_lr, 'R2': r2_lr}\n",
        "\n",
        "    # --- Modelo 2: Árvore de Decisão Regressora ---\n",
        "    print(\"\\nTreinando Modelo: Árvore de Decisão Regressora...\")\n",
        "    dt_model = DecisionTreeRegressor(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
        "    dt_model.fit(X_treino, y_treino)  # Árvores não requerem escalonamento\n",
        "    y_pred_dt = dt_model.predict(X_validacao)\n",
        "    mae_dt = mean_absolute_error(y_validacao, y_pred_dt)\n",
        "    rmse_dt = np.sqrt(mean_squared_error(y_validacao, y_pred_dt))\n",
        "    r2_dt = r2_score(y_validacao, y_pred_dt)\n",
        "    print(f\"Árvore de Decisão - MAE: {mae_dt:.2f}, RMSE: {rmse_dt:.2f}, R²: {r2_dt:.4f}\")\n",
        "    resultados_modelos['Árvore de Decisão'] = {'MAE': mae_dt, 'RMSE': rmse_dt, 'R2': r2_dt}\n",
        "\n",
        "    # --- Modelo 3: Random Forest Regressor ---\n",
        "    print(\"\\nTreinando Modelo: Random Forest Regressor...\")\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, max_depth=20, min_samples_split=5, min_samples_leaf=5, random_state=42, n_jobs=-1)\n",
        "    rf_model.fit(X_treino, y_treino)  # Árvores não requerem escalonamento\n",
        "    y_pred_rf = rf_model.predict(X_validacao)\n",
        "    mae_rf = mean_absolute_error(y_validacao, y_pred_rf)\n",
        "    rmse_rf = np.sqrt(mean_squared_error(y_validacao, y_pred_rf))\n",
        "    r2_rf = r2_score(y_validacao, y_pred_rf)\n",
        "    print(f\"Random Forest - MAE: {mae_rf:.2f}, RMSE: {rmse_rf:.2f}, R²: {r2_rf:.4f}\")\n",
        "    resultados_modelos['Random Forest'] = {'MAE': mae_rf, 'RMSE': rmse_rf, 'R2': r2_rf}\n",
        "\n",
        "    # --- Modelo 4: Gradient Boosting Regressor ---\n",
        "    print(\"\\nTreinando Modelo: Gradient Boosting Regressor...\")\n",
        "    gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, min_samples_split=5, min_samples_leaf=5, random_state=42)\n",
        "    gb_model.fit(X_treino, y_treino)  # Árvores não requerem escalonamento\n",
        "    y_pred_gb = gb_model.predict(X_validacao)\n",
        "    mae_gb = mean_absolute_error(y_validacao, y_pred_gb)\n",
        "    rmse_gb = np.sqrt(mean_squared_error(y_validacao, y_pred_gb))\n",
        "    r2_gb = r2_score(y_validacao, y_pred_gb)\n",
        "    print(f\"Gradient Boosting - MAE: {mae_gb:.2f}, RMSE: {rmse_gb:.2f}, R²: {r2_gb:.4f}\")\n",
        "    resultados_modelos['Gradient Boosting'] = {'MAE': mae_gb, 'RMSE': rmse_gb, 'R2': r2_gb}\n",
        "\n",
        "    # --- Resultados Consolidados ---\n",
        "    print(\"\\n--- Resultados Consolidados da Avaliação (no conjunto de validação) ---\")\n",
        "    df_resultados = pd.DataFrame(resultados_modelos).T\n",
        "    print(df_resultados.sort_values(by='RMSE'))\n",
        "else:\n",
        "    print(\"\\nModelos não foram treinados pois os dados de treino/validação (X, y) não estão definidos corretamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Média de Weekly_Sales no treino:\", y_treino.mean())\n",
        "print(\"Desvio padrão de Weekly_Sales no treino:\", y_treino.std())\n",
        "print(\"Média de Weekly_Sales na validação:\", y_validacao.mean())\n",
        "print(\"Desvio padrão de Weekly_Sales na validação:\", y_validacao.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VdIraEgWSsa"
      },
      "source": [
        "## Ajuste do Código para Análise de Importância das Features (Regressão Linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xDRRc0xSWY-o",
        "outputId": "10bf174e-55f5-4439-c5ef-e948bc693748"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "print(\"--- Análise de Importância das Features (Regressão Linear) ---\")\n",
        "\n",
        "# 1. Verificar se o modelo de Regressão Linear está treinado\n",
        "if 'lr_model' not in locals() or lr_model is None:\n",
        "    print(\"Modelo de Regressão Linear não encontrado. Treinando um novo modelo...\")\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_treino, y_treino)  # Assumindo que X_treino e y_treino estão definidos\n",
        "\n",
        "# 2. Verificar se X_treino está definido\n",
        "if 'X_treino' not in locals() or X_treino is None:\n",
        "    print(\"ERRO: X_treino não está definido. Não é possível prosseguir com a análise de importância.\")\n",
        "else:\n",
        "    # 3. Extrair os coeficientes do modelo de Regressão Linear\n",
        "    coeficientes = lr_model.coef_\n",
        "    feature_names = X_treino.columns\n",
        "\n",
        "    if len(coeficientes) == len(feature_names):\n",
        "        # 4. Criar um DataFrame para os coeficientes\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Coefficient': coeficientes,\n",
        "            'Abs_Coefficient': np.abs(coeficientes)  # Usar valor absoluto para comparar importância\n",
        "        })\n",
        "\n",
        "        # 5. Ordenar as features pela importância (valor absoluto dos coeficientes)\n",
        "        feature_importance_df = feature_importance_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "        # 6. Exibir as 15 features mais importantes\n",
        "        print(\"\\nImportância das Features (Top 15, Regressão Linear):\")\n",
        "        print(feature_importance_df[['Feature', 'Coefficient', 'Abs_Coefficient']].head(15))\n",
        "\n",
        "        # 7. Plotar a importância das features (Top 20)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        top_n_features = 20\n",
        "        sns.barplot(\n",
        "            x='Abs_Coefficient',\n",
        "            y='Feature',\n",
        "            data=feature_importance_df.head(top_n_features),\n",
        "            hue='Feature',\n",
        "            palette='viridis',\n",
        "            legend=False\n",
        "        )\n",
        "        plt.title(f'Top {top_n_features} Features Mais Importantes (Regressão Linear)')\n",
        "        plt.xlabel('Magnitude do Coeficiente (Importância)')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"ERRO: O número de coeficientes não corresponde ao número de nomes de features.\")\n",
        "        print(f\"Coeficientes: {len(coeficientes)}, Nomes de Features: {len(feature_names)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adicionar esta seção após o treinamento dos modelos, mas antes dos resultados consolidados\n",
        "\n",
        "# Avaliação no Conjunto de Treino\n",
        "resultados_treino = {}\n",
        "\n",
        "# Regressão Linear\n",
        "y_pred_lr_treino = lr_model.predict(X_treino_scaled)\n",
        "mae_lr_treino = mean_absolute_error(y_treino, y_pred_lr_treino)\n",
        "rmse_lr_treino = np.sqrt(mean_squared_error(y_treino, y_pred_lr_treino))\n",
        "r2_lr_treino = r2_score(y_treino, y_pred_lr_treino)\n",
        "resultados_treino['Regressão Linear'] = {'MAE': mae_lr_treino, 'RMSE': rmse_lr_treino, 'R2': r2_lr_treino}\n",
        "\n",
        "# Árvore de Decisão\n",
        "y_pred_dt_treino = dt_model.predict(X_treino)\n",
        "mae_dt_treino = mean_absolute_error(y_treino, y_pred_dt_treino)\n",
        "rmse_dt_treino = np.sqrt(mean_squared_error(y_treino, y_pred_dt_treino))\n",
        "r2_dt_treino = r2_score(y_treino, y_pred_dt_treino)\n",
        "resultados_treino['Árvore de Decisão'] = {'MAE': mae_dt_treino, 'RMSE': rmse_dt_treino, 'R2': r2_dt_treino}\n",
        "\n",
        "# Random Forest\n",
        "y_pred_rf_treino = rf_model.predict(X_treino)\n",
        "mae_rf_treino = mean_absolute_error(y_treino, y_pred_rf_treino)\n",
        "rmse_rf_treino = np.sqrt(mean_squared_error(y_treino, y_pred_rf_treino))\n",
        "r2_rf_treino = r2_score(y_treino, y_pred_rf_treino)\n",
        "resultados_treino['Random Forest'] = {'MAE': mae_rf_treino, 'RMSE': rmse_rf_treino, 'R2': r2_rf_treino}\n",
        "\n",
        "# Gradient Boosting\n",
        "y_pred_gb_treino = gb_model.predict(X_treino)\n",
        "mae_gb_treino = mean_absolute_error(y_treino, y_pred_gb_treino)\n",
        "rmse_gb_treino = np.sqrt(mean_squared_error(y_treino, y_pred_gb_treino))\n",
        "r2_gb_treino = r2_score(y_treino, y_pred_gb_treino)\n",
        "resultados_treino['Gradient Boosting'] = {'MAE': mae_gb_treino, 'RMSE': rmse_gb_treino, 'R2': r2_gb_treino}\n",
        "\n",
        "# Exibir resultados no conjunto de treino\n",
        "print(\"\\n--- Resultados Consolidados da Avaliação (no conjunto de treino) ---\")\n",
        "df_resultados_treino = pd.DataFrame(resultados_treino).T\n",
        "print(df_resultados_treino.sort_values(by='RMSE'))\n",
        "\n",
        "# Comparar com os resultados no conjunto de validação\n",
        "print(\"\\n--- Comparação Treino vs Validação (R²) ---\")\n",
        "comparacao_r2 = pd.DataFrame({\n",
        "    'R2_Treino': df_resultados_treino['R2'],\n",
        "    'R2_Validação': df_resultados['R2'],\n",
        "    'Diferença': df_resultados_treino['R2'] - df_resultados['R2']\n",
        "})\n",
        "print(comparacao_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaTHAjgzuQDQ"
      },
      "source": [
        "## Treinamento do Modelo Final (RandomForestRegressor v2) e Previsões no Conjunto de Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXN3Cd5dwPMW",
        "outputId": "d1b9c2dd-5000-4826-e318-0412236f1c57"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"--- Treinamento do Modelo Final e Previsões no Teste ---\")\n",
        "\n",
        "# 1: Definir as colunas de features e preparar X_full_train, y_full_train, X_teste\n",
        "print(\"\\n--- Preparando Dados para Treinamento Final ---\")\n",
        "\n",
        "if 'df_train_merged' not in locals() or 'df_test_merged' not in locals():\n",
        "    print(\"ERRO: df_train_merged ou df_test_merged não estão definidos. Interrompendo.\")\n",
        "    X_full_train, y_full_train, X_teste = None, None, None\n",
        "else:\n",
        "    # Codificação da coluna 'Type'\n",
        "    print(\"\\n--- Codificando a Coluna 'Type' ---\")\n",
        "    type_mapping = {'A': 3, 'B': 2, 'C': 1}\n",
        "    df_train_merged['Type'] = df_train_merged['Type'].map(type_mapping).astype('int32')\n",
        "    df_test_merged['Type'] = df_test_merged['Type'].map(type_mapping).astype('int32')\n",
        "\n",
        "    # Verificar se há outros valores não mapeados\n",
        "    print(\"Valores únicos em 'Type' após mapeamento (df_train_merged):\", df_train_merged['Type'].unique())\n",
        "    print(\"Valores únicos em 'Type' após mapeamento (df_test_merged):\", df_test_merged['Type'].unique())\n",
        "\n",
        "    colunas_a_excluir = ['Weekly_Sales', 'Date']\n",
        "    \n",
        "    colunas_features = [col for col in df_train_merged.columns if col not in colunas_a_excluir]\n",
        "    colunas_features = [f for f in colunas_features if not pd.api.types.is_datetime64_any_dtype(df_train_merged[f])]\n",
        "\n",
        "    print(f\"Total de features selecionadas: {len(colunas_features)}\")\n",
        "    if len(colunas_features) < 5:\n",
        "        print(f\"AVISO: Poucas features selecionadas ({len(colunas_features)}). Verifique 'colunas_features'. Features: {colunas_features}\")\n",
        "\n",
        "    X_full_train = df_train_merged[colunas_features].copy()\n",
        "    y_full_train = df_train_merged['Weekly_Sales'].copy()\n",
        "    \n",
        "    # Preparar X_teste, garantindo consistência de colunas com X_full_train\n",
        "    X_teste = df_test_merged.copy()\n",
        "    \n",
        "    # Adicionar colunas faltantes em X_teste com valor 0\n",
        "    for col in X_full_train.columns:\n",
        "        if col not in X_teste.columns:\n",
        "            print(f\"Adicionando coluna faltante '{col}' em X_teste com valor 0.\")\n",
        "            X_teste[col] = 0\n",
        "            \n",
        "    # Garantir que X_teste tenha apenas as colunas de X_full_train e na mesma ordem\n",
        "    try:\n",
        "        X_teste = X_teste[X_full_train.columns]\n",
        "        print(\"Colunas de X_teste alinhadas com X_full_train.\")\n",
        "    except KeyError as e:\n",
        "        print(f\"ERRO ao alinhar colunas de X_teste: {e}\")\n",
        "        print(\"Algumas colunas esperadas em X_full_train não puderam ser encontradas ou criadas em X_teste.\")\n",
        "        print(f\"Colunas em X_full_train: {list(X_full_train.columns)}\")\n",
        "        print(f\"Colunas em X_teste antes do alinhamento: {list(df_test_merged.copy().columns)}\")\n",
        "        X_teste = None\n",
        "\n",
        "    if X_teste is not None:\n",
        "        print(f\"\\nShape de X_full_train: {X_full_train.shape}\")\n",
        "        print(f\"Shape de y_full_train: {y_full_train.shape}\")\n",
        "        print(f\"Shape de X_teste: {X_teste.shape}\")\n",
        "    else:\n",
        "        print(\"\\nFalha na preparação de X_teste.\")\n",
        "\n",
        "# 2: Verificação e Tratamento de NaNs em X_full_train e X_teste\n",
        "if X_full_train is not None and X_teste is not None:\n",
        "    print(\"\\n--- Verificando e Tratando NaNs Remanescentes ---\")\n",
        "    # Calcular medianas do TREINO para CPI e Unemployment\n",
        "    cpi_fill_value = X_full_train['CPI'].median() if 'CPI' in X_full_train.columns else 0\n",
        "    unemp_fill_value = X_full_train['Unemployment'].median() if 'Unemployment' in X_full_train.columns else 0\n",
        "\n",
        "    for df_X, nome_df in zip([X_full_train, X_teste], ['X_full_train', 'X_teste']):\n",
        "        print(f\"\\nProcessando NaNs em {nome_df}:\")\n",
        "        nan_check = df_X.isnull().sum()\n",
        "        colunas_com_nan = nan_check[nan_check > 0]\n",
        "        \n",
        "        if not colunas_com_nan.empty:\n",
        "            print(f\"Colunas com NaNs em {nome_df} antes do preenchimento:\")\n",
        "            print(colunas_com_nan)\n",
        "            print(f\"Preenchendo NaNs restantes em {nome_df}...\")\n",
        "            for col_nan in colunas_com_nan.index:\n",
        "                if col_nan == 'CPI':\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(cpi_fill_value)\n",
        "                elif col_nan == 'Unemployment':\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(unemp_fill_value)\n",
        "                else:\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(0)\n",
        "            print(f\"NaNs preenchidos em {nome_df}. Nova verificação:\")\n",
        "            print(df_X.isnull().sum()[df_X.isnull().sum() > 0])\n",
        "        else:\n",
        "            print(f\"Nenhum NaN encontrado em {nome_df}.\")\n",
        "\n",
        "# 3: Verificar Infinitos\n",
        "if X_full_train is not None and X_teste is not None:\n",
        "    print(\"\\n--- Verificando Infinitos ---\")\n",
        "    # Evitar cópias desnecessárias ao verificar infinitos\n",
        "    numeric_cols_train = X_full_train.select_dtypes(include=np.number).columns\n",
        "    inf_check_train = pd.Series(0, index=numeric_cols_train)\n",
        "    for col in numeric_cols_train:\n",
        "        inf_check_train[col] = np.isinf(X_full_train[col]).sum()\n",
        "    print(f\"Infinitos em X_full_train: {inf_check_train.sum()}\")\n",
        "\n",
        "    numeric_cols_test = X_teste.select_dtypes(include=np.number).columns\n",
        "    inf_check_test = pd.Series(0, index=numeric_cols_test)\n",
        "    for col in numeric_cols_test:\n",
        "        inf_check_test[col] = np.isinf(X_teste[col]).sum()\n",
        "    print(f\"Infinitos em X_teste: {inf_check_test.sum()}\")\n",
        "\n",
        "# 4: Otimizar Tipos de Dados para Reduzir Uso de Memória\n",
        "if X_full_train is not None and X_teste is not None:\n",
        "    print(\"\\n--- Otimizando Tipos de Dados para Reduzir Uso de Memória ---\")\n",
        "    for df_X, nome_df in zip([X_full_train, X_teste], ['X_full_train', 'X_teste']):\n",
        "        for col in df_X.columns:\n",
        "            if df_X[col].dtype == 'int64':\n",
        "                df_X[col] = df_X[col].astype('int32')\n",
        "            elif df_X[col].dtype == 'float64':\n",
        "                df_X[col] = df_X[col].astype('float32')\n",
        "            elif df_X[col].dtype == 'bool':\n",
        "                df_X[col] = df_X[col].astype('bool')  # Já otimizado, mas para consistência\n",
        "        print(f\"Tipos de dados otimizados em {nome_df}:\")\n",
        "        print(df_X.dtypes.head())\n",
        "\n",
        "# 5: Treinar o Modelo Final (Random Forest)\n",
        "if X_full_train is not None and y_full_train is not None and X_teste is not None:\n",
        "    print(\"\\n--- Treinando o Modelo Final ---\")\n",
        "    \n",
        "    print(\"Modelo Escolhido: Random Forest Regressor\")\n",
        "    final_model = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1  # Usa todos os núcleos disponíveis para acelerar o treinamento\n",
        "    )\n",
        "    final_model.fit(X_full_train, y_full_train)\n",
        "    print(\"Treinamento do modelo final concluído.\")\n",
        "\n",
        "    # 6: Fazer Previsões no Conjunto de Teste\n",
        "    print(\"\\n--- Fazendo Previsões no Conjunto de Teste ---\")\n",
        "    predicoes_finais_teste = final_model.predict(X_teste)\n",
        "    print(\"Previsões no conjunto de teste concluídas.\")\n",
        "    print(f\"Número de previsões geradas: {len(predicoes_finais_teste)}\")\n",
        "    print(\"Exemplo das primeiras 5 previsões:\", predicoes_finais_teste[:5])\n",
        "\n",
        "    # 7: Opcional - Importância das Features\n",
        "    print(\"\\n--- Importância das Features ---\")\n",
        "    importancias = pd.DataFrame({\n",
        "        'Feature': X_full_train.columns,\n",
        "        'Importância': final_model.feature_importances_\n",
        "    })\n",
        "    print(importancias.sort_values(by='Importância', ascending=False).head(10))\n",
        "else:\n",
        "    print(\"\\nTreinamento do modelo e previsões não podem ser realizados devido a erros na preparação dos dados.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "condicao_de_parada = True # Defina sua condição\n",
        "if condicao_de_parada:\n",
        "    print(\"Parando a execução aqui.\")\n",
        "    raise SystemExit(\"Execução interrompida propositalmente.\")\n",
        "print(\"Isso não será executado se a condição acima for verdadeira.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Previsões no teste (Validação do modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Permutation Importance "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n--- Calculando Importância por Permutação (Regressão Linear) ---\")\n",
        "\n",
        "# Verifique se as variáveis necessárias existem\n",
        "if 'final_model' not in locals():\n",
        "    print(\"ERRO: Modelo 'final_model' não está definido. Interrompendo.\")\n",
        "elif 'X_validacao' not in locals() or 'y_validacao' not in locals():\n",
        "    print(\"AVISO: X_validacao ou y_validacao não estão definidos. Recriando a partir de df_train_merged...\")\n",
        "    if 'df_train_merged' not in locals():\n",
        "        print(\"ERRO: df_train_merged não está definido. Não é possível recriar X_validacao e y_validacao.\")\n",
        "    else:\n",
        "        # Recriar X_validacao e y_validacao com divisão temporal (80/20)\n",
        "        df_train_merged = df_train_merged.sort_values('Date')\n",
        "        train_size = int(len(df_train_merged) * 0.8)\n",
        "        df_validacao = df_train_merged.iloc[train_size:].copy()\n",
        "        y_validacao = df_validacao['Weekly_Sales'].copy()\n",
        "        colunas_a_excluir = ['Weekly_Sales', 'Date']\n",
        "        colunas_features = [col for col in df_validacao.columns if col not in colunas_a_excluir]\n",
        "        colunas_features = [f for f in colunas_features if not pd.api.types.is_datetime64_any_dtype(df_validacao[f])]\n",
        "        X_validacao = df_validacao[colunas_features].copy()\n",
        "        print(f\"X_validacao e y_validacao recriados com sucesso. Shape de X_validacao: {X_validacao.shape}\")\n",
        "\n",
        "# Verificar e tratar NaNs em X_validacao (se necessário)\n",
        "if 'X_validacao' in locals() and X_validacao is not None:\n",
        "    print(\"\\n--- Verificando e Tratando NaNs em X_validacao ---\")\n",
        "    nan_check = X_validacao.isnull().sum()\n",
        "    colunas_com_nan = nan_check[nan_check > 0]\n",
        "    if not colunas_com_nan.empty:\n",
        "        print(\"Colunas com NaNs em X_validacao antes do preenchimento:\")\n",
        "        print(colunas_com_nan)\n",
        "        print(\"Preenchendo NaNs em X_validacao...\")\n",
        "        # Usar medianas de X_full_train para preenchimento (se disponível)\n",
        "        if 'X_full_train' in locals():\n",
        "            cpi_fill_value = X_full_train['CPI'].median() if 'CPI' in X_full_train.columns else 0\n",
        "            unemp_fill_value = X_full_train['Unemployment'].median() if 'Unemployment' in X_full_train.columns else 0\n",
        "            for col_nan in colunas_com_nan.index:\n",
        "                if col_nan == 'CPI':\n",
        "                    X_validacao[col_nan] = X_validacao[col_nan].fillna(cpi_fill_value)\n",
        "                elif col_nan == 'Unemployment':\n",
        "                    X_validacao[col_nan] = X_validacao[col_nan].fillna(unemp_fill_value)\n",
        "                else:\n",
        "                    X_validacao[col_nan] = X_validacao[col_nan].fillna(0)\n",
        "        else:\n",
        "            X_validacao = X_validacao.fillna(0)\n",
        "        print(\"NaNs preenchidos em X_validacao. Nova verificação:\")\n",
        "        print(X_validacao.isnull().sum()[X_validacao.isnull().sum() > 0])\n",
        "    else:\n",
        "        print(\"Nenhum NaN encontrado em X_validacao.\")\n",
        "\n",
        "# Verificar se X_validacao_scaled existe; se não, criar com StandardScaler\n",
        "if 'X_validacao_scaled' not in locals() or X_validacao_scaled is None:\n",
        "    print(\"\\nAVISO: X_validacao_scaled não está definido. Escalonando X_validacao...\")\n",
        "    if 'scaler' not in locals():\n",
        "        print(\"AVISO: Scaler não encontrado. Criando um novo scaler a partir de X_full_train...\")\n",
        "        if 'X_full_train' not in locals():\n",
        "            print(\"ERRO: X_full_train não está definido. Não é possível escalonar X_validacao.\")\n",
        "        else:\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(X_full_train)  # Ajustar o scaler com X_full_train\n",
        "            X_validacao_scaled = scaler.transform(X_validacao)  # Escalonar X_validacao\n",
        "            print(\"Escalonamento de X_validacao concluído.\")\n",
        "    else:\n",
        "        X_validacao_scaled = scaler.transform(X_validacao)  # Usar o scaler existente\n",
        "        print(\"Escalonamento de X_validacao concluído com scaler existente.\")\n",
        "\n",
        "# Calcular a importância por permutação no conjunto de validação (escalonado)\n",
        "if 'X_validacao_scaled' in locals() and X_validacao_scaled is not None:\n",
        "    try:\n",
        "        perm_importance = permutation_importance(\n",
        "            final_model, \n",
        "            X_validacao_scaled,  # Usar X_validacao escalonado\n",
        "            y_validacao, \n",
        "            n_repeats=5, \n",
        "            random_state=42, \n",
        "            n_jobs=-1,\n",
        "            scoring='r2'  # Usar R² como métrica para consistência com regressão\n",
        "        )\n",
        "\n",
        "        # Organizar os resultados em um DataFrame\n",
        "        sorted_idx = perm_importance.importances_mean.argsort()  # Índices ordenados\n",
        "\n",
        "        perm_importance_df = pd.DataFrame(\n",
        "            data=perm_importance.importances_mean[sorted_idx],\n",
        "            index=X_validacao.columns[sorted_idx],  # Nomes das features na ordem correta\n",
        "            columns=['Importance_Permutation']\n",
        "        ).sort_values(by='Importance_Permutation', ascending=False)\n",
        "\n",
        "        # Exibir as 15 features mais importantes\n",
        "        print(\"\\nImportância das Features por Permutação (Top 15, Regressão Linear):\")\n",
        "        print(perm_importance_df.head(15))\n",
        "\n",
        "        # Plotar as 20 features mais importantes\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.barplot(\n",
        "            x='Importance_Permutation',\n",
        "            y=perm_importance_df.head(20).index,\n",
        "            data=perm_importance_df.head(20),\n",
        "            palette='viridis'\n",
        "        )\n",
        "        plt.title(\"Top 20 Features Mais Importantes (Permutation Importance - Regressão Linear)\")\n",
        "        plt.xlabel(\"Queda Média na Performance (R²)\")\n",
        "        plt.ylabel(\"Feature\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO ao calcular permutation importance: {e}\")\n",
        "else:\n",
        "    print(\"ERRO: X_validacao_scaled não está definido. Não é possível calcular permutation importance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partial Dependence Plots (PDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"\\n--- Gerando Partial Dependence Plots (Regressão Linear) ---\")\n",
        "\n",
        "# Verifique se as variáveis necessárias existem\n",
        "if 'final_model' not in locals() or 'X_full_train' not in locals() or not isinstance(X_full_train, pd.DataFrame):\n",
        "    print(\"ERRO: Variáveis 'final_model' ou 'X_full_train' não estão definidas ou X_full_train não é um DataFrame. Interrompendo.\")\n",
        "else:\n",
        "    # Exibir colunas disponíveis para depuração\n",
        "    print(f\"Colunas disponíveis em X_full_train: {X_full_train.columns.tolist()}\")\n",
        "\n",
        "    # Escalonar X_full_train, mantendo nomes das colunas\n",
        "    if 'X_full_train_scaled' not in locals() or X_full_train_scaled is None:\n",
        "        print(\"AVISO: X_full_train_scaled não está definido. Escalonando X_full_train...\")\n",
        "        if 'scaler' not in locals():\n",
        "            print(\"AVISO: Scaler não encontrado. Criando um novo scaler...\")\n",
        "            scaler = StandardScaler()\n",
        "            X_full_train_scaled = scaler.fit_transform(X_full_train)\n",
        "        else:\n",
        "            print(\"Reutilizando scaler existente...\")\n",
        "            X_full_train_scaled = scaler.transform(X_full_train)\n",
        "        # Converter X_full_train_scaled para DataFrame para preservar nomes das colunas\n",
        "        X_full_train_scaled = pd.DataFrame(X_full_train_scaled, columns=X_full_train.columns)\n",
        "        print(\"Escalonamento de X_full_train concluído.\")\n",
        "\n",
        "    # Escolha features para PDP com base nas mais importantes (ajustadas para colunas reais)\n",
        "    features_para_pdp = [\n",
        "        'IsHoliday',    # Já existe\n",
        "        'Size',         # Já existe\n",
        "        'CPI',          # Já existe\n",
        "        'Weekly_Sales_lag_1_x',  # Substitui roll_mean_4 por lag_1 como proxy\n",
        "        'Weekly_Sales_lag_4_x',  # Substitui roll_std_4 por lag_4 como proxy\n",
        "        'Weekly_Sales_lag_52_x'  # Substitui roll_mean_52 por lag_52\n",
        "    ]\n",
        "    \n",
        "    # Filtrar para garantir que apenas features existentes em X_full_train sejam usadas\n",
        "    features_para_pdp_existentes = [f for f in features_para_pdp if f in X_full_train.columns]\n",
        "    \n",
        "    if not features_para_pdp_existentes:\n",
        "        print(\"ERRO: Nenhuma das features selecionadas para PDP foi encontrada em X_full_train.\")\n",
        "        print(f\"Colunas disponíveis: {X_full_train.columns.tolist()}\")\n",
        "    else:\n",
        "        print(f\"Gerando PDPs para: {features_para_pdp_existentes}\")\n",
        "        \n",
        "        # Determinar o número de linhas e colunas para os subplots\n",
        "        n_features_pdp = len(features_para_pdp_existentes)\n",
        "        n_cols_pdp = 2 if n_features_pdp > 1 else 1\n",
        "        n_rows_pdp = (n_features_pdp + n_cols_pdp - 1) // n_cols_pdp\n",
        "        \n",
        "        # Ajustar o tamanho da figura dinamicamente\n",
        "        fig_height = 4 * n_rows_pdp\n",
        "        fig_width = 6 * n_cols_pdp\n",
        "        \n",
        "        fig, ax = plt.subplots(n_rows_pdp, n_cols_pdp, figsize=(fig_width, fig_height), squeeze=False)\n",
        "        ax = ax.flatten()\n",
        "\n",
        "        # Gerar PDP para cada feature selecionada\n",
        "        try:\n",
        "            display_pdp = PartialDependenceDisplay.from_estimator(\n",
        "                final_model,\n",
        "                X_full_train_scaled,  # Usar dados escalonados com nomes de colunas\n",
        "                features=features_para_pdp_existentes,\n",
        "                kind='average',\n",
        "                n_jobs=-1,\n",
        "                grid_resolution=20,\n",
        "                ax=ax[:n_features_pdp],\n",
        "                feature_names=X_full_train.columns.tolist()  # Explicitamente passar nomes das colunas\n",
        "            )\n",
        "            \n",
        "            # Remover eixos não utilizados\n",
        "            for i in range(n_features_pdp, len(ax)):\n",
        "                fig.delaxes(ax[i])\n",
        "            \n",
        "            # Ajustar título e layout\n",
        "            plt.suptitle(\"Partial Dependence Plots (PDP) para Features Selecionadas (Regressão Linear)\", fontsize=16, y=1.01)\n",
        "            plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"ERRO ao gerar os PDPs: {e}\")\n",
        "            print(\"Verifique se as features em 'features_para_pdp_existentes' são adequadas para PDP.\")\n",
        "            print(f\"Features selecionadas: {features_para_pdp_existentes}\")\n",
        "            print(f\"Colunas disponíveis em X_full_train: {X_full_train.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliar Overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"--- Avaliando Overfitting: Performance no Treino vs. Validação ---\")\n",
        "\n",
        "# Dicionário para armazenar os resultados\n",
        "resultados_treino = {}\n",
        "resultados_validacao = {}\n",
        "\n",
        "# Função auxiliar para calcular e imprimir métricas\n",
        "def calcular_metricas(nome_modelo, modelo, X, y_real, sufixo_dataset=\"\"):\n",
        "    y_pred = modelo.predict(X)\n",
        "    mae = mean_absolute_error(y_real, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_real, y_pred))\n",
        "    r2 = r2_score(y_real, y_pred)\n",
        "    print(f\"{nome_modelo} - {sufixo_dataset} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}\")\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
        "\n",
        "# Recriar X_treino, y_treino, X_validacao e y_validacao se necessário\n",
        "if 'X_treino' not in locals() or 'y_treino' not in locals() or 'X_validacao' not in locals() or 'y_validacao' not in locals():\n",
        "    print(\"\\nAVISO: X_treino, y_treino, X_validacao ou y_validacao não estão definidos. Recriando a partir de df_train_merged...\")\n",
        "    if 'df_train_merged' not in locals():\n",
        "        print(\"ERRO: df_train_merged não está definido. Não é possível recriar os dados.\")\n",
        "    else:\n",
        "        # Divisão temporal 80/20\n",
        "        df_train_merged = df_train_merged.sort_values('Date')\n",
        "        train_size = int(len(df_train_merged) * 0.8)\n",
        "        df_treino = df_train_merged.iloc[:train_size].copy()\n",
        "        df_validacao = df_train_merged.iloc[train_size:].copy()\n",
        "        y_treino = df_treino['Weekly_Sales'].copy()\n",
        "        y_validacao = df_validacao['Weekly_Sales'].copy()\n",
        "        colunas_a_excluir = ['Weekly_Sales', 'Date']\n",
        "        colunas_features = [col for col in df_treino.columns if col not in colunas_a_excluir]\n",
        "        colunas_features = [f for f in colunas_features if not pd.api.types.is_datetime64_any_dtype(df_treino[f])]\n",
        "        X_treino = df_treino[colunas_features].copy()\n",
        "        X_validacao = df_validacao[colunas_features].copy()\n",
        "        print(f\"Dados recriados com sucesso. Shape de X_treino: {X_treino.shape}, Shape de X_validacao: {X_validacao.shape}\")\n",
        "\n",
        "# Verificar e tratar NaNs em X_treino e X_validacao\n",
        "if 'X_treino' in locals() and 'X_validacao' in locals():\n",
        "    for df_X, nome_df in zip([X_treino, X_validacao], ['X_treino', 'X_validacao']):\n",
        "        nan_check = df_X.isnull().sum()\n",
        "        colunas_com_nan = nan_check[nan_check > 0]\n",
        "        if not colunas_com_nan.empty:\n",
        "            print(f\"\\nColunas com NaNs em {nome_df} antes do preenchimento:\")\n",
        "            print(colunas_com_nan)\n",
        "            print(f\"Preenchendo NaNs em {nome_df}...\")\n",
        "            cpi_fill_value = X_treino['CPI'].median() if 'CPI' in X_treino.columns else 0\n",
        "            unemp_fill_value = X_treino['Unemployment'].median() if 'Unemployment' in X_treino.columns else 0\n",
        "            for col_nan in colunas_com_nan.index:\n",
        "                if col_nan == 'CPI':\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(cpi_fill_value)\n",
        "                elif col_nan == 'Unemployment':\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(unemp_fill_value)\n",
        "                else:\n",
        "                    df_X[col_nan] = df_X[col_nan].fillna(0)\n",
        "            print(f\"NaNs preenchidos em {nome_df}. Nova verificação:\")\n",
        "            print(df_X.isnull().sum()[df_X.isnull().sum() > 0])\n",
        "\n",
        "# Escalonar os dados, já que o modelo foi treinado com dados escalonados\n",
        "if 'X_treino' in locals() and 'X_validacao' in locals():\n",
        "    if 'scaler' not in locals():\n",
        "        print(\"\\nAVISO: Scaler não encontrado. Criando um novo scaler a partir de X_treino...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_treino_scaled = scaler.fit_transform(X_treino)\n",
        "        X_validacao_scaled = scaler.transform(X_validacao)\n",
        "    else:\n",
        "        print(\"\\nReutilizando scaler existente...\")\n",
        "        X_treino_scaled = scaler.transform(X_treino)\n",
        "        X_validacao_scaled = scaler.transform(X_validacao)\n",
        "    print(\"Escalonamento concluído.\")\n",
        "\n",
        "# --- Avaliação no Conjunto de Treino ---\n",
        "print(\"\\n--- Performance no Conjunto de Treino ---\")\n",
        "if 'final_model' in locals() and 'X_treino_scaled' in locals() and 'y_treino' in locals():\n",
        "    resultados_treino['Regressão Linear'] = calcular_metricas('Regressão Linear', final_model, X_treino_scaled, y_treino, \"Treino\")\n",
        "else:\n",
        "    print(\"ERRO: final_model, X_treino_scaled ou y_treino não estão definidos.\")\n",
        "\n",
        "# --- Avaliação no Conjunto de Validação ---\n",
        "print(\"\\n--- Performance no Conjunto de Validação ---\")\n",
        "if 'final_model' in locals() and 'X_validacao_scaled' in locals() and 'y_validacao' in locals():\n",
        "    resultados_validacao['Regressão Linear'] = calcular_metricas('Regressão Linear', final_model, X_validacao_scaled, y_validacao, \"Validação\")\n",
        "else:\n",
        "    print(\"ERRO: final_model, X_validacao_scaled ou y_validacao não estão definidos.\")\n",
        "\n",
        "# --- Comparação Direta ---\n",
        "print(\"\\n--- Comparativo Treino vs. Validação ---\")\n",
        "if resultados_treino and resultados_validacao:\n",
        "    df_comp_treino = pd.DataFrame(resultados_treino).T.add_suffix('_Treino')\n",
        "    df_comp_validacao = pd.DataFrame(resultados_validacao).T.add_suffix('_Validação')\n",
        "    \n",
        "    df_comparativo_overfitting = pd.concat([df_comp_treino, df_comp_validacao], axis=1)\n",
        "    # Reordenar para melhor visualização\n",
        "    cols_r2 = [col for col in df_comparativo_overfitting.columns if 'R2' in col]\n",
        "    cols_rmse = [col for col in df_comparativo_overfitting.columns if 'RMSE' in col]\n",
        "    cols_mae = [col for col in df_comparativo_overfitting.columns if 'MAE' in col]\n",
        "    \n",
        "    df_comparativo_overfitting = df_comparativo_overfitting[cols_r2 + cols_rmse + cols_mae]\n",
        "    display(df_comparativo_overfitting)\n",
        "else:\n",
        "    print(\"Não foi possível gerar o comparativo. Verifique se as métricas de treino e validação foram calculadas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faZgr-wt9jY1"
      },
      "source": [
        "# Gerando arquivo treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "e7yB3tkZ9jD2",
        "outputId": "c79b9ceb-4f16-4aba-d849-2324df4a5778"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # Certifique-se de que o pandas está importado\n",
        "\n",
        "print(\"--- Preparando Arquivo de Submissão ---\")\n",
        "\n",
        "# 1. Criar a coluna 'Id' no formato esperado (Store_Dept_Date)\n",
        "# A coluna 'Date' em df_test_merged deve ser do tipo datetime.\n",
        "# Vamos formatá-la como string 'YYYY-MM-DD' para criar o Id.\n",
        "df_test_submission = df_test_merged.copy() # Trabalhar em uma cópia para não alterar o df_test_merged original\n",
        "df_test_submission['Date_str'] = df_test_submission['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "df_test_submission['Id'] = df_test_submission['Store'].astype(str) + '_' + \\\n",
        "                           df_test_submission['Dept'].astype(str) + '_' + \\\n",
        "                           df_test_submission['Date_str']\n",
        "\n",
        "# 2. Criar o DataFrame de submissão com as colunas 'Id' e 'Weekly_Sales'\n",
        "# A coluna 'Weekly_Sales' aqui conterá suas previsões\n",
        "df_submissao = pd.DataFrame({\n",
        "    'Id': df_test_submission['Id'],\n",
        "    'Weekly_Sales': predicoes_finais_teste  # Suas previsões finais do modelo\n",
        "})\n",
        "\n",
        "# 3. Arredondar as previsões para um número razoável de casas decimais, se desejar\n",
        "# (Muitas competições não exigem, mas pode ser bom para consistência)\n",
        "# df_submissao['Weekly_Sales'] = df_submissao['Weekly_Sales'].round(4)\n",
        "\n",
        "\n",
        "# 4. Salvar o arquivo de submissão com o nome desejado\n",
        "nome_arquivo_submissao = 'random_forest_predictions_walmart.csv'\n",
        "df_submissao.to_csv(nome_arquivo_submissao, index=False)\n",
        "\n",
        "print(f\"\\nArquivo de submissão '{nome_arquivo_submissao}' criado com sucesso.\")\n",
        "\n",
        "# 5. Exibir as primeiras linhas do arquivo de submissão para verificação\n",
        "print(\"\\nExemplo das primeiras 5 linhas do arquivo de submissão:\")\n",
        "display(df_submissao.head())\n",
        "\n",
        "# Verificar o shape do arquivo de submissão\n",
        "print(f\"\\nShape do arquivo de submissão: {df_submissao.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMSR0ntAqXx"
      },
      "source": [
        "# Graficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XOV_ooSCLuF2",
        "outputId": "06d9380d-8511-437a-cfe5-87a9c430ef85"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "print(\"\\n--- Gerando Gráficos de Validação para o Modelo de Regressão Linear ---\")\n",
        "\n",
        "# Verificar se as variáveis necessárias existem\n",
        "if 'final_model' not in locals() or 'X_validacao_scaled' not in locals() or 'y_validacao' not in locals():\n",
        "    print(\"ERRO: Variáveis 'final_model', 'X_validacao_scaled' ou 'y_validacao' não estão definidas. Interrompendo.\")\n",
        "elif 'df_validacao' not in locals():\n",
        "    print(\"ERRO: df_validacao não está definido. Interrompendo.\")\n",
        "else:\n",
        "    # Gerar previsões com final_model\n",
        "    y_pred = final_model.predict(X_validacao_scaled)\n",
        "\n",
        "    # Calcular R² para usar no título\n",
        "    r2 = r2_score(y_validacao, y_pred)\n",
        "\n",
        "    # Calcular os resíduos\n",
        "    residuos = y_validacao - y_pred\n",
        "\n",
        "    # 1. Gráfico de Previsões vs. Valores Reais (Scatter Plot)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_validacao, y_pred, alpha=0.5, s=10)\n",
        "    plt.plot([y_validacao.min(), y_validacao.max()], [y_validacao.min(), y_validacao.max()], 'r--', lw=2, label='Linha de Perfeição')\n",
        "    plt.xlabel('Valores Reais (Weekly_Sales)')\n",
        "    plt.ylabel('Valores Previstos')\n",
        "    plt.title(f'Previsões vs. Valores Reais (Regressão Linear)\\nR²: {r2:.4f}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Gráfico de Resíduos vs. Valores Previstos\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_pred, residuos, alpha=0.5, s=10)\n",
        "    plt.axhline(y=0, color='r', linestyle='--', lw=2, label='Resíduo = 0')\n",
        "    plt.xlabel('Valores Previstos (Weekly_Sales)')\n",
        "    plt.ylabel('Resíduos')\n",
        "    plt.title('Resíduos vs. Valores Previstos (Regressão Linear)')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Distribuição dos Resíduos (Histograma + KDE)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(residuos, kde=True, stat='density')\n",
        "    plt.axvline(x=0, color='r', linestyle='--', lw=1, label='Resíduo = 0')\n",
        "    plt.xlabel('Resíduos')\n",
        "    plt.ylabel('Densidade')\n",
        "    plt.title('Distribuição dos Resíduos (Regressão Linear)')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Previsões ao Longo do Tempo\n",
        "    colunas_temporais = ['DayOfYear', 'WeekOfYear', 'Month', 'Year', 'Quarter']\n",
        "    coluna_temporal = 'Date' if 'Date' in df_validacao.columns else next((col for col in colunas_temporais if col in X_validacao.columns), None)\n",
        "\n",
        "    # Filtrar para uma loja ou departamento específico\n",
        "    mask = None\n",
        "    if 'Store' in df_validacao.columns and 'Dept' in df_validacao.columns:\n",
        "        mask = (df_validacao['Store'] == 1) & (df_validacao['Dept'] == 1)\n",
        "        df_validacao_filtered = df_validacao[mask].copy()\n",
        "        y_validacao_filtered = y_validacao[mask]\n",
        "        y_pred_filtered = y_pred[mask]\n",
        "\n",
        "        if len(df_validacao_filtered) == 0:\n",
        "            print(\"AVISO: Nenhum dado encontrado para Store=1 e Dept=1. Mostrando dados sem filtro.\")\n",
        "            df_validacao_filtered = df_validacao\n",
        "            y_validacao_filtered = y_validacao\n",
        "            y_pred_filtered = y_pred\n",
        "    else:\n",
        "        print(\"AVISO: Colunas 'Store' ou 'Dept' não encontradas em df_validacao. Mostrando dados sem filtro.\")\n",
        "        df_validacao_filtered = df_validacao\n",
        "        y_validacao_filtered = y_validacao\n",
        "        y_pred_filtered = y_pred\n",
        "\n",
        "    if coluna_temporal:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        if coluna_temporal == 'Date':\n",
        "            plt.plot(df_validacao_filtered['Date'], y_validacao_filtered, label='Valores Reais', alpha=0.7)\n",
        "            plt.plot(df_validacao_filtered['Date'], y_pred_filtered, label='Valores Previstos', alpha=0.7)\n",
        "            plt.fill_between(df_validacao_filtered['Date'], y_validacao_filtered, y_pred_filtered, color='gray', alpha=0.2, label='Diferença')\n",
        "            plt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))\n",
        "            plt.xticks(rotation=45)\n",
        "        else:\n",
        "            if mask is not None:\n",
        "                X_validacao_filtered = X_validacao[mask]\n",
        "            else:\n",
        "                X_validacao_filtered = X_validacao\n",
        "            plt.plot(X_validacao_filtered[coluna_temporal], y_validacao_filtered, label='Valores Reais', alpha=0.7)\n",
        "            plt.plot(X_validacao_filtered[coluna_temporal], y_pred_filtered, label='Valores Previstos', alpha=0.7)\n",
        "            plt.fill_between(X_validacao_filtered[coluna_temporal], y_validacao_filtered, y_pred_filtered, color='gray', alpha=0.2, label='Diferença')\n",
        "            plt.xticks(rotation=45)\n",
        "        plt.xlabel(coluna_temporal)\n",
        "        plt.ylabel('Weekly_Sales')\n",
        "        plt.title(f'Previsões vs. Valores Reais ao Longo de {coluna_temporal} (Regressão Linear)')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"AVISO: Nenhuma coluna temporal encontrada. Usando índice como proxy.\")\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(y_validacao_filtered, label='Valores Reais', alpha=0.7)\n",
        "        plt.plot(y_pred_filtered, label='Valores Previstos', alpha=0.7)\n",
        "        plt.fill_between(range(len(y_validacao_filtered)), y_validacao_filtered, y_pred_filtered, color='gray', alpha=0.2, label='Diferença')\n",
        "        plt.xlabel('Índice (Proxy Temporal)')\n",
        "        plt.ylabel('Weekly_Sales')\n",
        "        plt.title('Previsões vs. Valores Reais ao Longo do Tempo (Regressão Linear)')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
